\chapter{WiNoCoD Project and Wired OFDMA Based RF Interconnect } % Main chapter title

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 3. \emph{WiNoCoD Project and Wired OFDMA Based RF Interconnect}} % This is for the header on each page - perhaps a shortened title

In Section 2.3, we review the state-of-the-art propositions for 1000-core interconnects. Wired RF distinguishes itself with its viability with today's technology due to CMOS compatibility \cite{chang2008cmp}. Both of the proposed RF and optical proposals seem to provide necessary high bandwidth and scalable power budget, however there exists a significant problem. These architectures rely on utilization of dedicated \textit{hardware circuitry} to create orthogonal channels, i.e. as many dedicated set of hardware circuits as number of channels. Optical interconnects require ring resonators, filters, modulators for composing channels on different wavelengths. Whereas, in RF frequencies, electrical CMOS devices such as local oscillators, filters and mixers are used. There are two important drawbacks with this approach. First of all, intra-chip bandwidth demand keeps increasing in parallel with increasing core count, which results in the quadratic increase for these circuits to implant on-chip. Obviously, this tendency is not scalable in terms of area, power and budget. Next, we see that this type of FDMA (or WDMA) implementation is not reconfigurable or encloses a very limited reconfigurability. In other words, bandwidth can not be distributed among nodes, according to instantaneous traffic demands. We presented significant spatio-temporal heterogeneity of on-chip traffic in Section 2.4. ATAC presented in Section 2.3.1.1, uses a SWMR scheme which is not reconfigurable due to fact that modulators for each channel cannot be implanted in every transmitter \cite{kurian2010atac}. Even for the architectures with limited reconfigurability such as \cite{chang2008cmp}, the bandwidth granularity is really low and channel allocation is done in analog level by switching circuitry which is not rapid and effective. Due to these constraints, state-of-the-art RF and optical architectures are dimensioned for the worst-case traffic, i.e. an on-chip node is guaranteed to have bandwidth which can sustain its peak traffic. However, we know that for most of the time traffic intensity for on-chip nodes are far lower than this peak, thus making this scheme non-efficient due to redundant bandwidth allocation \cite{soteriou2006statistical}. Besides, the only logical scheme supporting broadcast -\textit{which is the most vital requirement for scalable cache coherence protocols}- SWMR is prohibitive in energy and not preferred.

To overcome all of these on-chip drawbacks and provide the necessary breakthrough with bandwidth reconfigurability, in 2012, WiNoCoD project (Wired RF Based Network on Chip Reconfigurable on Demand) is initiated by the partnership of ANR, ETIS-ENSEA, LIP6 laboratories, Supelec-IETR, NXP Semiconductors. At the heart of the project lies the revolutionary Orthogonal Frequency Division Multiple Access (OFDMA) based RF Interconnect for a NoC, which provides \textit{digital level}, high granularity, rapid and effortless bandwidth allocation.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.6\textwidth, height= 0.5\textwidth]{./Chapter3_Figures/Chapter3_01_v2.pdf}
    \rule{35em}{0.5pt}
  \caption[WiNoCoD Project]{WiNoCoD is a project funded by French National Research Agency (ANR) with the aim of implementing the first OFDMA RF interconnect backed massive multiprocessor, with the partnership of ETIS-ENSEA laboratories, IETR-CentraleSupelec, UPMC-LIP6 laboratories and NXP Semiconductors}
  \label{fig:Electron}
\end{figure}

\section{WiNoCoD On-Chip Architecture}


WiNoCoD provisions a 2048 core generic massive CMP. A shared memory principle is adopted, so that the address space is accessible by all processing elements such as in TSAR architecture developed by LIP6 laboratory partner (Section 2.2.3). As we have mentioned previously, in general in Section 2.1, placing a single RAM block with a dedicated single memory controller in one part of the multiprocessor is not scalable for the future CMP systems with hundreds, thousands of cores \cite{blagodurov2010case}. In order to alleviate this effect, researchers have developed the \textit{Non-Uniform Memory Architecture} (NUMA) principle \cite{kurian2010atac}. WiNoCoD utilizes this type of a memory architecture, where physical RAM of the system is partitioned over the chip, but any core can reach any part of the memory, through the sharing of a general address space. WiNoCoD architecture is composed of 512 tiles, including 4 processors and 2 Gbytes of RAM on each tile. Project plans to employ 1 TBytes of memory in total, which is divided in to uniform 512 pieces, 2 Gbytes of RAM slices. Our CMP is arranged in 3 different hierarchical levels due to sustain scalability and modularity, with each of its level containing a dedicated different NoC infrastructure (Fig. 3.2). These choices, as well as the RF based topology will be explained later. This approach provides a full modular scalability, so that new tiles can be added if needed in future.   

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.79\textwidth]{./Chapter3_Figures/Chapter3_02_v2.pdf}
    \rule{35em}{0.5pt}
  \caption[3 level hierarchy of WiNoCoD architecture]{3 level hierarchy of WiNoCoD architecture incorporating 2048 cores. Each level has a different dedicated NoC infrastructure}
  \label{fig:Electron}
\end{figure}

\subsection{Need for independent communication layers}

We have mentioned previously that a standard electrical NoC lacks the scalability to sustain the interconnection of hundreds of cores, especially due to high number of routers to traverse. Particularly taking our architecture as an example, if 2048 cores would be connected by a standard 2D mesh, we could have envisaged a topology configured as 64x32 mesh. Therefore, for the communication between the two farthest cores, a packet would have to traverse 96 routers. Assuming few cycles of processing latency in each router, one can see that this will lead to prohibitively large communication delays, even under low traffic load. And additionally, even under a low traffic intensity, the mesh network shall be congested easily, causing the saturation of the interconnection. However, using RF to interconnect every one or few cores is not scalable also. As we will highlight the details on the RF interconnection in next sections, the necessary transceivers lead to significant amount of power consumption and surface area. For instance, the estimated surface area of a core is $0.35 \, mm^{2}$ and it has been estimated by other WiNoCoD's other partners that the necessary RF transceiver is $1.17 \, mm^{2}$. Moreover, the estimated power consumption of a core is $39 \, mW$ and the power consumption of an RF transceiver is $309.5 \, mW$. We can understand the ineffectiveness of employing an RF transceiver for each few number of cores, from these figures. In WiNoCoD, 2048 cores are provisioned, so that we make a hypothetical comparison for different number of RF transceivers, to understand the need of a hierarchical structure. Table 3.1 shows the percentage of required surface area and power consumption for all required RF transceivers and their percentages compared to total core and memory area and power consumption; assuming different number of cores are grouped together to be served by the RF interconnect. For instance, if 2 cores are grouped together, we need 1024 transceivers and if 1024 cores grouped together we need only 2 transceivers. However note that, this assumption does not include the area of the waveguide, which spans a considerable surface area, increasing substantially by increasing number of RF accesses, as we will mention in incoming sections. 

From Table 3.1, we see that if each core has a transceiver, the total required surface area for RF transceivers consitute 75.53\% of total area including surface area of cores and memory, and consume 88.09\% of total power, which is quite unacceptable. Even if each 16 cores are grouped to use RF interconnect, the transceivers span 16.36\% of total area and consume 31.62\% of total power. In WiNoCoD, 64 cores are grouped at the highest layer to communicate with RF interconnect, where it corresponds to 4.84\% of total surface area and 10.36\% of total power consumption. These figures justify this choice. Considering all of these, WiNoCoD employs a 3-level hierarchical architecture, where each layer has a special type of interconnection.   

\begin{table}[]
\centering
\caption{Estimated total surface area and power consumption for different number of RF transceivers, compared to total area and power consumption of 2048 cores and 1 TByte RAM.}
\label{my-label}
\begin{tabular}{|l|l|l|l|l|}
\hline
\begin{tabular}[c]{@{}l@{}}Number of \\ aggregated\\ cores to \\ communicate\\ with RF.\end{tabular} & \begin{tabular}[c]{@{}l@{}}Total area of\\ required RF \\ transceivers.\\ ($mm^{2}$)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Percentage of \\ total area of\\ required RF \\ transceivers\\ compared to\\ total area of\\ cores and RAM\end{tabular} & \begin{tabular}[c]{@{}l@{}}Total power of\\ required RF \\ transceivers.\\ (W)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Percentage of \\ total power of\\ required RF \\ transceivers\\ compared to\\ total power of\\ cores and RAM\end{tabular} \\ \hline
1                                                                                              & 2396.16                                                                                                        & 75.53\%                                                                                                                                            & 633.85 & 88.09\%                                                                                                                                              \\ \hline
4                                                                                              & 599.04                                                                                                         & 38.16\%                                                                                                                                             & 158.46 & 64.91\%                                                                                                                                              \\ \hline
16                                                                                             & 149.76                                                                                                         & 16.36\%                                                                                                                                             & 39.62 & 31.62\%                                                                                                                                               \\ \hline
64                                                                                             & 37.44                                                                                                          & 4.84\%                                                                                                                                              & 9.90                                                                                         & 10.36\%                                                                                                                                               \\ \hline
256                                                                                            & 9.36                                                                                                          & 1.25\%                                                                                                                                             & 2.48                                                                                         & 2.80\%                                                                                                                                               \\ \hline
1024                                                                                           & 2.34                                                                                                          & 0.31\%                                                                                                                                             & 0.62 & 0.71\%                                                                                                                                               \\ \hline
\end{tabular}
\end{table}



\subsection{3-level hierarchy}

In this section, we present the 3 different modular hierarchical level of WiNoCoD, each with its dedicated type of NoC infrastructure. To demonstrate the feasibility of the proposed design, the surface and power consumption estimation is done \cite{briere2015dynamically}. 22 nm technology is targeted to be utilized in the project, and based on the equations given in \cite{semiconductor2013itrs}, we estimate the normalized surface and power by using the characteristics of technology used in state-of-the-art components. For instance, the surface and power of a target technology's gate length $N$ can be estimated by using the values from reference technology's gate length $Q$, with equations below, respectively :

\begin{align}
	S_{N} = S_{Q} \, (\frac{N}{Q})^{2}
\end{align}

\begin{align}
	P_{N} = P_{Q} \, 0.65^{log_{0.7}(N/Q)}
\end{align} 

For instance, as we are targeting a technology of 22 nm gate length for processors, we have estimated the power and surface by using the ARM Cortex-A5 specifiactions which is built with $Q=32$ nm gate length \cite{sewell2012swizzle}. FFT/IFFT blocks are based on \cite{sung2009reconfigurable} and ADC/DAC circuits are based on \cite{chan20123}, both with a target technology of 180 nm gate length. 
    
\subsubsection{A Tile}

A \textit{tile} is our lowest hierarchical entity where each of them incorporates 4 processing elements (cores) (each with their dedicated L1 instruction and data caches), a uniform portion of the total RAM (2 GBytes) and a Direct Memory Access (DMA) controller. All of these components share the same address space and are connected by a standard local crossbar. Crossbars provide a low latency and simultaneous communication, but their performance degrades drastically after the number of connected nodes increase a few. Therefore number of elements accessing to crossbar is limited to 7 in our architecture including 4 cores, memory directory, RAM slice and the mesh NoC router. We assume a 22 nm technology and a standard 32-bit core. However, our architecture is independent of the processor type, that any existing standard processors such as MIPS32, PPC 405, SPARC V8 etc. can be used. Each core has an estimated surface area of $0.35 \, mm^{2}$ and power consumption of $39 \, mW$. 2 GBytes of RAM slices in each tile has an estimated surface area of $0.03 \, mm^{2}$ and a power consumption of $11.29 \, mW$. At the end total tile surfarce area is $0.91 \, mm^{2}$ with all other elements including crossbar, routers, memory controller etc. \cite{briere2014winocod}. Each tile has a mesh router which is connected to the crossbar at one end, where all elements in the tile can reach, that they can access the 2D mesh network. Note that, the tile structure is based on the previous TSAR project (Section 2.2.3).

\subsubsection{A Tileset}

Next hierarchical element in our architecture is a \textit{tileset} which is composed of 16 tiles where they are interconnected with a conventional electrical 4x4 2D mesh network. The packets travel using a simple x-y routing with virtual channels. Each router (except edges) has 4 different interfaces to the adjacent tiles' routers each with a dedicated transmitting and receiving buffer. The nodes in tiles access this level, by a router and wrapper inside the tile. As at this level we switch to packetized NoC paradigm, the necessary fragmentation and defragmentation is done by these entities. We have mentioned the viability of a 2D electrical mesh network with tens of nodes in Section 2.2. Concerning our provisioned latency constraint, the number of nodes in a tileset is limited to 16. There are 32 tilesets in WiNoCoD CMP, each with an access to RF transmission line. With all its elements, a tileset has an estimated surface area of $14.62 \, mm^{2}$ \cite{briere2014winocod}. 


\subsubsection{Inter-tileset Communication}

The most distinctive feature of our CMP architecture is its OFDMA based RF interconnect. A core who wants to transmit a message to a memory controller in another tileset (and vice versa), sends the message via crossbar to the mesh NoC, and finally using the mesh to the RF transceiver. The message is propagated on the RF transmission line and reach the RF transceiver of the destined tileset. Following this, using the reverse path through the mesh network and crossbar, it reaches the terminal position. We will describe the enabling technology and the details of our OFDMA based RF interconnect in following sections. Finally, with all its components including transmission line and RF transecivers, whole estimated surface area of our CMP is $476.83 \, mm^{2}$ and a total power consumption of $95.27 \, W$ \cite{briere2015dynamically}. 

Table 3.2 sums up the values for estimated surface area and power consumption for certain elements in our CMP.


\begin{table}[]
\centering
\caption{Estimated surface area and power consumption for certain elements in CMP}
\label{Estimated surface area and power consumption for certain elements in CMP}
\begin{tabular}{|c|c|c|}
			& Area $(mm^{2})$    & Power (mW)     \\ \hline	
Core                    & 0.35    & 39      \\ \hline
Tile (4 cores)          & 0.91    & 167.29  \\ \hline
Tileset (16 tiles)      & 14.62   & 2977.25 \\ \hline
RF Transceivers (x32)   & 1.17    & 309.5   \\ \hline
Transmission Line       & 8.71    & ---     \\ \hline
Total CMP (32 tilesets) & 476.83  & 95272   \\ \hline
\end{tabular}
\end{table}

\subsection{Details of Cache Coherence Protocol}

We have described the notion of cache coherence and its importance for multiprocessors in Section 2.1. NUMA type memory architecture explained previously, is the key to programmability of massive CMPs with thousands of cores \cite{blagodurov2010case}. An important design goal for a cache coherence protocol and shared memory architecture is to introduce a high \textit{usability}, which refers to the abstraction of hardware to the programmer \cite{manchanda2010non}. 

Our project draws many parallelism with the Tera-Scale Architecture (TSAR) project, which is another 1024-core generic shared NUMA memory CMP design, but without an RF interconnect as we have mentioned previously in Section 2.2.3 \cite{greiner2009tsar}. We employ a \textit{Directory Based Hybrid Cache Coherence Protocol} (DHCCP) in WiNoCoD, like in TSAR. A similar approach is adopted in 1024-core, optically interconnected ATAC \cite{kurian2010atac}. 

Each 2 GBytes of shared memory element in a tile (RAM slice) is associated with a \textit{directory} and access to this memory is regulated via a Direct Memory Access (DMA) unit. Thus, the cache coherence is hardware initiated. A \textit{Write Through} approach is adopted, where in case of a core want to write a data to an address line, it transmits a write request to the directory responsible for the corresponding address line. We have mentioned previously in Section 2.1, that these type of write request messages contain \textit{cache lines}, which is including the raw data for computation. The address space for 1 TBytes, where each line is represented with 40 bits, is evenly divided among 512 tilesets. A directory in a tileset is responsible for the address lines spanned in its interval. Therefore, for instance, if the intended address line is spanned by the directory in the same tileset, the message does not go outside tile, and only uses crossbar to reach DMA. Similarly, if the adress line is in a different tile in the same tileset, it only uses the mesh network to reach. If the address line is in a tile of a different tileset, it has to use RF interconnect. Hence, exploiting spatial locality in memory programming is essential to decrease traffic load in higher hierarchies. After receiving the write request, firstly the directory checks whether other cores have a copy of this address line. If so, it transmits \textit{invalidate} message to the sharers, so they stop using the copy of this address line in their L1 caches, as it will change by the modification of the core who wants to write a new content to this address line \cite{greiner2009tsar}. This procedure is illustrated in Fig. 3.3, where CPU-A sends a write request for ADDRESS-X; CPU-B and CPU-C have a copy of it. The directory in charge of this address line, DIR sends invalidation messages to them.

In contrast, for the \textit{Write Back} policy, when the core updates its L1 cache, the main memory (directory) is updated only when the modified cache line is updated. Even though write through policy is more bandwidth intensive, write back policy is preferred due to its scalability.

Similarly for reading data, L1 caches of cores gather the copy of the main memory from the responsible directory and DMA of the associated address line. In a sense, we can state that the distributed main memory is a L2 cache for the cores.

A directory is a simple register, containing data associated to each cache line it is responsible. For each cache line, it contains the IDs of the cores who has a copy of it. And of course, in the associated line in RAM, it has the original raw data. When we have thousands of cores, one can see that it is not possible for a directory to keep the IDs of hundreds or thousands of sharer cores, for each cache line. Depending on the length of a cache line, entries in a directory changes, however for instance for the case of 2048-cores and 64 bytes of cache lines, each directory in a tile with 512 Gbytes of RAM is responsible of 8 billion cache lines. Assuming ID of each core is represented by 10 bits, each of the 8 billion cache line in a directory should have a 256 Kbyte of memory, which is practically impossible.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.85\textwidth, height= 0.6\textwidth]{./Chapter3_Figures/Chapter3_03_v2.pdf}
    \rule{35em}{0.5pt}
  \caption[Illustration of a write request initiated by CPU-A in WiNoCoD, where the intended address is currently shared by 2 other cores.]{Illustration of a write request initiated by a core in WiNoCoD, where the intended address is currently shared by 2 other cores (CPU-B and CPU-C).}
  \label{fig:Electron}
\end{figure}

In order to alleviate this, distributed hybrid cache coherence protocol (DHCCP) is adopted in WiNoCoD, as in \cite{greiner2009tsar}\cite{kurian2010atac}. When the number of sharers exceeds a certain threshold (for instance 8), the directory keeps the \textit{number of sharers}, rather than ID of each sharing core. The directory in a tileset is depicted in Fig. 3.4. If the number of sharers are lower than the threshold, in case of an invalidation, directory transmits invalidation messages to each of the sharing cores. However, if this threshold is exceeded, directory simply broadcasts this invalidation message to all 2048 cores. Then, it counts the number of acknowledgement messages from the intended cores and validates it by comparing to the number of sharers it holds in the register. This bandwidth intensive cache coherence protocol is a price to pay for the scalability of future massive CMPs. One can understand the quest of developing an effective reconfigurable and broadcast capable interconnect such as in WiNoCoD, due to this cache coherency protocol. 
   
\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.7\textwidth, height= 0.35\textwidth]{./Chapter3_Figures/Chapter3_04.pdf}
    \rule{35em}{0.5pt}
  \caption[Memory directory in a tileset]{Implementation of memory directory in a tileset. With hybrid cache coherence protocol, for each cache line in the RAM, if the number of cores are lower than a threshold, sharer core IDs are explicitly registered, if not, only number of sharers are stored.}
  \label{fig:Electron}
\end{figure}
   

\section{Basics of OFDMA}

In this section, we explain the basics of OFDMA technology along with its possible benefits for on-chip interconnects. OFDMA is a medium access scheme based on Orthogonal Frequency Division Multiplexing (OFDM).

\subsection{OFDM}

Orthogonal Frequency Division Multiplexing (OFDM) is a modulation technique that transforms a large bandwidth signal into many, orthogonal narrow band channels and encodes digital information on frequency domain rather than time domain. Since its first standardization in 1995 for Digital Audio Broadcasting (DAB) \cite{bruno1994digital}, it has conquered and revolutionized all fields of digital telecommunications and became the popular physical layer solution due to its numerous advantages \cite{zhouofdm}. Main advantage is its capability to resist to multipath issue, thanks to a simplified equalization. However, most appealing advantage for on-chip application is the capability to multiplex several transmissions in the same signal with a high level of flexibility and dynamicity.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.8\textwidth]{./Chapter3_Figures/Chapter3_05.pdf}
    \rule{35em}{0.5pt}
  \caption[Transmission and reception chain of an OFDM modulator/demodulator]{Transmission and reception chain of an OFDM modulator/demodulator}
  \label{fig:Electron}
\end{figure}


OFDM is remarkably different than other conventional FDM systems, due to its method of generating orthogonal channels. As stated previously, in OFDM, a large band signal (high rate) is decomposed to many parallel narrow band (lower rate) symbols. To implement this, the digital data is first mapped to constellation symbols such as BPSK, QPSK, M-QAM etc. and each constellation symbol is associated with a \textit{subcarrier}. A subcarrier is the atomic frequency unit in an OFDM signal, or in other words it is one of these parallel narrow band signals. Then an Inverse Discrete Frequency Transform (IDFT) is applied to the parallel vector of $N$ subcarriers, where each of them is now a complex number associated with the encoded constellation symbol (each complex number represents a certain number of bits). In order to perform the frequency transformation rapid, a Fast Fourier Transform (FFT) or Inverse Fast Fourier Transform (IFFT) is applied \cite{james1967historical}. The result of this transform gives again a vector of $N$ complex numbers. Following, this vector of N points is serialized and converted to a time-domain signal with the appropriate data rate. This resulting signal is called as an \textit{OFDM symbol}, and unless stated else we will refer it as a \textit{symbol} throughout the rest of this thesis. Fig. 3.5 shows the transmission and reception chain of a typical OFDM modulator-demodulator. At reception the exact inverse of the aforementioned procedures are followed to decode the received OFDM symbol, whereas FFT is used, instead of IFFT at the TX. The time and frequency representation of an OFDM symbol can be illustrated as in Fig. 3.6. Loosely speaking, in the frequency axis each subcarrier (orthogonal channel) of OFDM symbol can be seen as a superposition of sinusoids with different amplitudes, phases and frequencies. It is important to remark that an OFDM symbol is the atomic decodable unit, in this system. The whole OFDM symbol should be decoded to extract the data, as data are encoded not in time, but frequency domain. 

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.8\textwidth]{./Chapter3_Figures/OFDM_Signal_v3.pdf}
    \rule{35em}{0.5pt}
  \caption[Representation of an OFDM symbol with duration \textit{T} both on frequency and time domain.]{Representation of an OFDM symbol with duration \textit{T} both on frequency and time domain.}
  \label{fig:Electron}
\end{figure}


  
At this point we shall make certain points clearer on mathematical basis of OFDM. As it can be seen from Fig 3.7, the frequency domain is spanned by subcarriers which are formulated by a \textit{sinc} function \cite{CarlosOFDM}. This comes from the fact that the sampling and transformation to analog domain are done with rectangular pulses, and the frequency transform of this gives a $sinc(x) = \frac{sin(x)}{x}$ function. Another advantage of OFDM roots from these sinc functions, as they are still orthogonal mathematically at the center of frequency spacings, even they overlap. This allows for the maximum bandwidth efficiency \cite{debbah2004short}. Reciprocal of the frequency interval between subcarriers $\Delta f$ gives an OFDM symbol's duration : $T = 1/\Delta f$.


\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.9\textwidth]{./Chapter3_Figures/Chapter3_07_v4.pdf}
    \rule{35em}{0.5pt}
  \caption[Subcarriers in frequency domain]{(a) Subcarriers in frequency domain are represented by \textit{sinc} function, as it is the frequency transformation of a rectangular interpolator which is used to convert digital samples to analog OFDM signal. (b) Even though sinc functions are superposed in frequency domain, they are still mathematically orthogonal at the center of frequency spacings. This gives the maximum spectral efficiency.}
  \label{fig:Electron}
\end{figure}



Note that, the resulting OFDM time domain analog signal still incorporates a vector of digital data, however coded on frequency domain rather than time domain. This property has numerous advantages. The multi-path effect in transmission channels is the result of multiple constructive and destructive copies of the transmitted signal superposed at receiver, distorting the content of the information. Thus an equalization is needed. It results in different gains through the spectrum, which is also referred as \textit{frequency selective channels}. For OFDM signals, this means different channel gains for each subcarrier, which can be mitigated by a simple multiplication with the inverse channel gain, as mentioned previously. Apart from equalization, even just a portion of digital data on subcarriers with relatively better channel gains can be recovered. 

\subsection{OFDMA}

Orthogonal Frequency Division Multiple Access (OFDMA), on the other hand is an OFDM based multiple access scheme, featuring all physical layer advantages of OFDM, along with an efficient bandwidth reconfigurability \cite{pietrzyk2006ofdma}. In parallel with OFDM, it gained popularity for the multiple user telecommunications standards. The key point of OFDMA is its powerful dynamic bandwidth reconfigurability, which allows assigning different subcarriers to different users, (thus data rate), on every symbol. A user only encodes information on its allocated subcarriers at transmission as explained previously, and nulls the rest (encodes \textit{zero information} before IFFT) to allow other users in the system access the medium. The encoding of digital data on allocated subcarriers is illustrated in Fig. 3.8 via a simple example, where TX-2 is allocated the 32 subcarriers between No. 32-64. 

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.95\textwidth]{./Chapter3_Figures/chapter3_07_v3.pdf}
    \rule{35em}{0.5pt}
  \caption[Encoding of digital data by TX-2 on its allocated subcarriers.]{Encoding of digital data by TX-2 on its allocated subcarriers, where it pads 0s to remaining subcarriers.}
  \label{fig:Electron}
\end{figure}


This way, contention resolution in the system is greatly simplified. Note that, this procedure can be implemented in \textit{digital domain}, i.e. by just manipulating the bit vector before the IFFT. This task can be performed by a simple microprocessor or a digital circuit. In addition, as all nodes in the system decode the same received OFDM symbol, they can receive the transmitted by all other nodes, thus OFDMA inherits an intrinsic broadcast capability. The logical transmission and reception between nodes in an OFDMA based medium (in case our RF interconnect) is illustrated in Fig. 3.9. For instance, TX-1 (transmitter of Tileset-1) encodes its data on its allocated subcarriers, TX-2 encodes its data on its allocated subcarriers etc. and RX-1 (receiver of Tileset-2), RX-2, etc. receive all transmissions from each tileset.  


\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.79\textwidth]{./Chapter3_Figures/Chapter3_09_v3.pdf}
    \rule{35em}{0.5pt}
  \caption[Communication between nodes in an OFDMA medium]{Communication between nodes in an OFDMA medium using different subcarriers. Note the intrinsic broadcast capability and nulled subcarriers.}
  \label{fig:Electron}
\end{figure}

In contrast to OFDMA, as explained in detail in Section 2.3 conventional FDMA (RF) and WDMA (optical) networks rely on static, pre-tuned CMOS or optical circuits to generate orthogonal channels. Only few of the proposals has a limited reconfigurability option to allocate these channels to nodes dynamically, which is unacceptable for the highly bursty on-chip traffic. In addition, as far as we know, a digital bandwidth allocation has not been introduced before to this field of RF-NoC, which allows designers to implement highly basic, adaptive, and rapid algorithms. Besides, the robustness and bandwidth efficiency of OFDM signals makes it a good candidate for interconnects. Considering the strong spatio-temporal heterogeneity of on-chip traffic and the vitality of multicast-broadcast cache coherence messages, we can better perceive the technological leap that OFDMA can bring to 1000-core CMPs. In a way, we can claim that OFDMA Interconnect implements a SWMR cross-bar which allows re-arbitrating the bandwidth for each transmitter on every symbol.  

\section{OFDMA Based RF Interconnect}

OFDM and OFDMA's potential was already provisioned when it was first theorized in 1957 \cite{mosier1958kineplex}. However at the time its implementation on field was limited to few military applications. This was due to the fact that the realization of the system was costly and not feasible with the technology of the era. Until 1980's, OFDM and OFDMA was not introduced to market, however it received attention from the academia and several articles and patents were issued. With exponentially increasing abundance of silicon resources, FFT chips have become a reality for OFDM and OFDMA to conquer the market and become the standard modulation of many recent communication standard. Examples of systems that use OFDM both on wire and wireless are ADSL, IEEE 802.11 (WLAN), 4G (LTE), digital TV and radio etc. With recent technology, even Ultra Wideband systems (UWB) \cite{saberinia2003pulsed}, or intra-data center OFDMA based optical interconnects \cite{ji2013first} are proposed.   


Considering that the time for enabling technologies to implement OFDMA on-chip has come too, WiNoCoD project was initiated to pioneer this field. We expect a similar tendency with the above mentioned standards, that OFDM shall be the preferable modulation for on-chip communication in near future. As wired RF is a feasible approach due to CMOS compatibility (Section 2.3), it is preferred for WiNoCoD's interconnection.

As mentioned in Section 3.1, in WiNoCoD chip, 32 tilesets are interconnected via an serpentine, cross-shaped RF transmission line for the inter-tileset communication as in Fig 3.2. Each RF interface in tilesets, has an OFDM modulator and demodulator. The packets that are generated inside a tile in tileset, which are destined to a tile in another tileset, traverses the electrical mesh network and reaches to the RF access point. Provided converter technology by our partner NXP envisions a 20 GHz bandwidth for the system. Based on the design constraints and circuit simulations, most suitable spectrum is chosen between 20-40 GHz \cite{hamieh2014sizing}. It is decided to have 1024 subcarriers, thus 1024-point FFT and IFFT blocks are required. Hence, as we have a 20 GHz bandwidth with 1024 subcarriers, we have subcarrier frequency spacing of $19.53 MHz$, where an OFDM symbol duration is $T = 1/19.53 \, MHz = 51.2 \,\, nanoseconds$. This value is approximated as 50 ns for ease of use throughout the manuscript. Table 3.2 summarizes these figures. One may question the choice for 1024 subcarriers in the system as there are only 32 tilesets are intended to use it. As number of subcarriers increase, the size of the FFT (and IFFT) increases, which at the end increases the complexity and computation time for the modules. More specifically, the complexity of FFT (IFFT) computation increases in $\mathcal{O}(log_{2}(N_{subcarriers}))$, with number of subcarriers \cite{hongwei2009fft}. Therefore, in a most basic sense, having FFT (IFFT) blocks of 32 would simplify the complexity 5 times compared to 1024-size FFT/IFFT modules. However, at the start of the project, 1024 subcarriers are dimensioned, which allow for a finer granularity of bandwidth. For instance, as we will investigate in Section 4.3.3.6, this resolution allows for the proper and effective signaling in our system. In addition, with further development the OFDMA based RF interconnect may need to sustain much more on-chip nodes, which require this fine granularity scheme for bandwidth partition.      


\begin{table}[]
\centering
\caption{Characteristic parameters of WiNoCoD's OFDMA interconnect}
\label{my-label}
\begin{tabular}{|l|l|l|l|}
\hline
Bandwidth                                                             & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Number \\ of \\ subcarriers\\ ($N_{subcarriers}$) \end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Sampling \\ Frequency \\ / Bandwidth \\ Per Subcarrier \\ ($\Delta f$) \end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Symbol \\ Duration\\  ($T = 1/(\Delta f)$)\end{tabular}} \\ \hline
\begin{tabular}[c]{@{}c@{}}20 GHz\\ (20-40 GHz spectrum)\end{tabular} 
& \begin{tabular}[c]{@{}c@{}} 1024 \end{tabular}                                                                                     
& \begin{tabular}[c]{@{}c@{}} 19.53 MHz                                                                                                       \end{tabular}                                                                                     
& \begin{tabular}[c]{@{}c@{}} 51.2 ns                                                                                       \end{tabular}                                                                                     
\\ \hline
\end{tabular}
\end{table}


With ever increasing demand for computational power in following decades, 100s of GHz of bandwidth shall be required for sustaining on-chip communication. In this thesis and WiNoCoD project, the operational bandwidth is restricted to 20 GHz, due to the currently feasible state-of-the-art technology provided by NXP. However, in parallel with developing silicon technology, one can expect for the on-chip OFDM bandwidth to increase, to keep up with the rising data rate demand. Actually, with its digital nature, reconfigurability and reliance on less amount of circuitry, we can designate the proposed on-chip OFDMA interconnect as a highly viable approach for the future trends. 

\subsection{RF Controller}
A packet that is going to be transmitted by a tileset through RF is welcomed primarily by the RF controller of this tileset. These packets (composed of flits) are processed if necessary -\textit{fragmented or defragmented, extracted or padded information such as source ID etc.} and inserted into the transmission queue. As mentioned previously, most important reason of using OFDMA on-chip is to reallocate bandwidth among different transmitters. There are various approaches and algorithms to allocate subcarriers and modulation orders among transmitters, through using a central arbiter or decentralized synchronous decisions. These methods which constitute the backbone of this thesis work, are explained in following chapters. Thus, RF controller makes certain decisions according to information such as queue state coming from the transmission buffer (TX) and the traffic information coming from other tilesets or a central intelligent unit, and changes the configuration of subcarriers, where it will encode data. With the modulation order on each subcarrier, this determines the throughput of the tileset on current symbol. As it is reviewed in Section 3.2.2, the data in TX buffer is fetched and encoded on dedicated subcarriers along with the chosen modulation order and the rest of the subcarriers are leaved idle. In addition, RF controller can be also made responsible of scheduling and sorting of packets, according to needs of architecture. Note that, this RF controller is completely digital thanks to OFDM, which allows a vast range of possibilities for physical and link layer actions. This intelligent module can be implemented as a simple single or multiple multiprocessors, or a basic digital circuit, based on requirements. 

This thesis work focuses on the utilization of the frequency resources of the proposed OFDMA interconnect optimally as possible, taking into account the constraints of the on-chip environment. The main goal is to reduce the waiting times in the transmission side queues as much as possible, as it is directly related to the allocated data rate (i.e. subcarriers) to a tileset. The state of the receiver side queues, or the intra-tileset communication (i.e. how flits transmit inside a tileset) is out of scope of this thesis. We assume that the intra-tileset mesh network can serve the packets in the receiver queues with a considerably higher rate compared to OFDMA transmission. In other words, the received packets can be sent to their destination tiles via the electrical mesh network with a constantly operating router. Without loss of generality, we assume that the packets do not have to wait for a long time to be transmitted. For instance, an OFDM symbol is approximately 50 ns and we can assume a router can serve one flit every ns. In addition, we do not also take into account the any other metric in the system other than transmission queue states to allocate the subcarriers, such as traffic in intra-tileset traffic etc..

\subsection{RF Front-end}

\subsubsection{Transmitter Side}

The number of flits to be transmitted by a tileset on waveguide, is fetched from the TX queue in the RF Controller, according to number of subcarriers (and modulation order) on that OFDM symbol. Then it is sent to RF Front-end which includes OFDM modulation. In Section 3.2, we have examined OFDM and OFDMA from an upper-layer perspective, abstracting over the underlying electronics. Now, we provide a more in-depth view. At first step, the bits to transmit are parallelized and mapped to constellation symbols with chosen modulation order. Constellation belongs to BPSK and M-QAM modulation. This is a basic encoding mechanism in digital communication : bits are represented by \textit{constellation symbols}, which can be formalized as a complex value, $a +jb$. Each of 2 indexes of this value, are the amplitude levels belonging to \textit{in phase} and \textit{quadrature} components. Then these constellation symbols are mapped on subcarriers. Rest of the subcarriers remain idle. Then IFFT gives a N element vector of complex values. These I/Q values are serialized and fed to two seperate Digital-to-Analog Converters (DAC). 

\begin{figure}[htbp]
  \centering
    \includegraphics[width=1.00\textwidth]{./Chapter3_Figures/Chapter3_08_v3.pdf}
    \rule{35em}{0.5pt}
  \caption[transmission and reception RF interface]{The detailed illustration of transmission and reception RF interface of a tileset.}
  \label{fig:Electron}
\end{figure}

The digital modelling and testing of the RF front-end is done by the thesis work which is also in collaboration with WiNoCoD project of A. Briere \cite{briere2015dynamically}. Two other PhD students working in the project, Frederic Drillet and Lounis Zerioul are responsible of the analog design, parametrisation and the simulation of the utilized CMOS components in the RF front-end. 

The up-conversion mixers combine a baseband signal with a local oscillator signal. Mixing occurs in a MOSFET, whose gate and drain are respectively fed by the local oscillator and the baseband signal. The higher frequency output is recovered in the MOSFET source. A drawback of such a device is the weak LO/RF isolation. It leads to a high power LO carrier in the output spectrum. As the local oscillator frequency is 30 GHz, which is the middle of our 20 GHz bandwidth, it needs to be suppressed. Thanks to the differential outputs of the DAC, two IQ-Modulators can work together to do so. Besides avoiding interference caused by image frequencies they can reduce the LO level in the output. As we use the same local oscillator for both of IQ-modulators and opposite I-Q signals, the IQ-Modulators outputs are subtracted in a differential amplifier to perform this suppression. Then this signal is amplified and transmitted on waveguide. This procedure is illustrated in Fig 3.10.      


Note that, this operation is done synchronously every T = 51.2 \, ns at every cluster's RF interface. At the end of this operation the resulting analog signal is the \textit{OFDM symbol}. All the analog components in the transceiver is projected to be manufactured in 250 nm SiGe technology for demonstration purposes. 

\subsubsection{Receiver Side}

The reception is done synchronously every T = 51.2 ns as transmission, too. The received signal from the transmission line is amplified and fed to a separator circuit, mixers and 30 GHz local oscillator to obtain in-phase and quadrature components. Then I and Q components are converted to digital domain by our Analog-to-Digital (ADC) components. After Serial to Parallel conversion this vector of I and Q values are converted to frequency domain by an FFT block. The resulting constellation symbols are demapped to bits, serialized and finally switched to the RF controller. The detailed reception chain of the RF Interface is shown in Fig. 3.10. Note that, the exact inverse of transmission operations is done to retrieve data.

Utilized FFT/IFFT processors are estimated to be manufactured with 22 nm CMOS technology. We estimate the area of each of these modules as 0.31 $mm^2$ and power consumption of $67.5 \, mW$. FFT/IFFT computation duration can be treated as a \textit{pipelined latency} through symbol-by-symbol communication. Each of ADCs and DACs are assumed to be 22 nm technology and have an estimated surface area of 0.12 $mm^2$ and power consumption of $81 \, mW$ \cite{briere2015dynamically}. 




\subsubsection{Transmission Line and Access}

M. Hamieh has developed and simulated transmission line and access for his thesis in collaboration with WiNoCoD Project \cite{hamieh2014sizing}. He has designed a cross-shape transmission line, to minimize the distance between two farthest tilesets in CMP, as in Fig. 3.2. In this configuration maximum distance between two nodes is 80 mm. Cross-shape design also gives an additional advantage, by providing a near flat frequency response over 20 GHz bandwidth. Based on the simulations, through the spectrum of 20-40 GHz, attenuation is measured in range of -40 dB and -50 dB between two farthest nodes and relatively non-varying with transistor based access \cite{hamieh2014sizing}. This simplifies the utilization of any subcarrier in the system without changing transmission power drastically and does not require equalization. 

A state-of-the-art silicon Microstrip Transmission Line delivered by NXP Semiconductors is used. The height of the transmission line is 8.78 \si\micro m with a permittivity of 4.2 and loss tangent of $2.10^{-4}$. In order to minimize the metal loss, characteristic impedance is determined as 30 \si\ohm. A new access mechanism to transmission line via transistors has been developed by M. Hamieh, preferred over existing capacitive coupling or direct access schemes. This new method reduces the frequency reflections phenomenon, frequency response fluctuations and attenuation compared to capacitive coupling and direct access \cite{hamieh2014sizing}. 

The linear attenuation through the transmission line is approximately 0.2-0.3 dB/mm. For instance, assuming a bit error rate of $10^{-8}$, the required minimum transmission power on a single subcarrier, between two farthest nodes is -39 dBm for BPSK, and -25 dBm for 64-QAM. Note that, the required power increases linearly with the number of used subcarriers \cite{hamieh2014sizing}.  Fig. 3.11 illustrates the proposed access method via transistors. In Fig. 3.12, the frequency response for the transmission between two farthest tilesets is shown for the newly proposed access mechanism via transistors, compared to two existing access schemes; direct access and capacitive coupling. As it can be seen from Fig. 3.12, the proposed access mechanism results in much lower attenuation, between -10 dB and -42 dB. As frequency increases the attenuation in dB increases linearly. We see that capacitive coupling provides a relatively flat frequency response, but with a much higher attenuation of approximately -60 dB.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.95\textwidth]{./Chapter3_Figures/fig_hamieh03.jpg}
    \rule{35em}{0.5pt}
  \caption[Proposed access via transistor mechanism for WiNoCoD.]{Proposed access via transistor mechanism for WiNoCoD (Image taken from \cite{hamieh2014sizing}).}
  \label{fig:Electron}
\end{figure}


\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.80\textwidth]{./Chapter3_Figures/fig_hamieh04.jpg}
    \rule{35em}{0.5pt}
  \caption[transmission and reception RF interface]{The frequency response between two farthest tileset on transmission line for the developed access mechanism via transistor compared to capacitive coupling and direct access (Image taken from \cite{hamieh2014sizing}).}
  \label{fig:Electron}
\end{figure}



\section{Conclusion}

In this chapter, we have presented the WiNoCoD project along its contributing partners and specified the intended 2048-core CMP architecture with the multi-level NoC. We have defined the lowest hierarchical level composed of 4 cores with a system RAM slice and responsible memory directory. Tilesets composed of 16 tiles, which are interconnect with a conventional electrical 2D mesh network has been presented. These tilesets are the highest hierarchical elements in the NoC, that they are interconnected by the wired RF network, therefore they constitute the main interest of focus for this thesis work. The necessary information is derived from the thesis work of A. Briere, which is also in collaboration with WiNoCoD project. 

Next, we have explained the scalable hybrid cache coherency protocol to be used in WiNoCoD. This is important for the goals of this thesis work, as cache coherency packets constitute the traffic circulating in the NoC infrastructure, which also includes the RF level.

Before presenting our OFDMA based RF interconnect, we have introduced the notion and basics of OFDM. Its revolutionary advantages have been listed. It is important to highlight the preliminary information on OFDM and OFDMA, as this project is to first to consider OFDMA for an on-chip CMP interconnect, to the best of our knowledge.

Then, we have specified the details of the RF Front-ends and utilized transmission line. The necessary information is derived from the thesis works of M. Hamieh, F. Drillet and L. Zerouil which are also in collaboration with WiNoCoD project. The rest of this thesis work focuses on the OFDMA based allocation issue for the WiNoCoD architecture, so that it is essential to based on the parameters and details of the wired RF infrastructure, we have seen in this chapter.     