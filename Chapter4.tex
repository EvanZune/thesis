% Chapter 4

\chapter{RF NoC Bandwidth Allocation Problem} % Main chapter title

\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 4. \emph{Understanding Bandwidth Scheduling}} % This is for the header on each page - perhaps a shortened title


In Chapter 3, we have presented the 2048-core architecture of WiNoCoD and features of its OFDMA based RF interconnect. As mentioned previously, using OFDMA for an RF interconnect is a pioneering attempt that would enable reconfiguring bandwidth effectively and rapidly among tilesets, based on their changing demands. The purpose of this chapter is to define the bandwidth demands of tilesets, in other words cache-coherency packets created by each tileset which are destined to locations in other tilesets to use this RF interconnect, more precisely from a queuing theory perspective. Each symbol, each 1024 subcarrier of the interconnect, can be regarded as an element that can serve these packets. Briefly, this thesis work aims to develop strong algorithms which allocate subcarriers dynamically to tilesets effectively, while respecting the nanosecond scale constraints of the architecture, thanks to the high bandwidth reconfigurability of OFDMA. Therefore in this chapter, firstly we formulate this bandwidth allocation problem we encounter, considering physical constraints of WiNoCoD and main metrics to improve in terms of on-chip requirements. Then, we review the vast literature on similar bandwidth allocation problems, belonging to diverse fields of cellular communications to processor scheduling. Lastly, the required concepts and notions which we use for the bandwidth scheduling in WiNoCoD, are explained in detail. In a nutshell, this chapter forms a theoretical preliminary basis for our bandwidth allocation algorithms, which we propose in following chapters.      


\section{WiNoCoD's OFDMA RF Interconnect }


\subsection{Motivation}

WiNoCoD's innovative OFDMA RF interconnect aims to improve the performance of the network-on-chip laid on a 2048-core CMP architecture, by changing users of subcarriers dynamically. Recalling the specifications given in previous chapter, there are 32 tilesets, where each contain 64 cores. We have seen that each tileset has an access point to transmission line via its OFDMA modulator. Thanks to the intrinsic broadcast capability of OFDMA, any broadcast/multicast packet need not to be multiplied, as transmissions by a user are received by all others (SWMR). This shall increase system capacity drastically considering the requirement of large number of broadcast/multicast packets of a threshold based hybrid directory based cache coherency system. In previous section, we have seen that there exists a transmission buffer where packets coming from intra-tileset mesh network which are destined to other tilesets are stored, and served by OFDMA RF interconnect in a FIFO manner. Similarly a receiver buffer exists at the end of the OFDMA demodulator, receiving all transmissions from other tilesets. Wrappers process headers of these transmissions and store packets which are destined in a tile in its tileset. 

A subcarrier on an OFDM symbol can be seen as an element, serving transmission of information of 1,2 etc. bits based on the utilized modulation order BPSK, QPSK etc. As mentioned previously, primary motivation behind using OFDMA on-chip is its high reconfigurability, which refers to the ability of changing owners of subcarriers over time. Therefore main goal shall be to exploit this reconfigurability as much as possible, concerning unique constraints of our OFDMA interconnect and unorthodox requirements of on-chip traffic. Hence, our problem can be formulated as allocating subcarriers in different time instants (i.e. one or several OFDM symbols) to different transmission queues (Fig. 4.1). Assuming that receiver side queues can be served with a constant high rate, (which means that received packets can be transmitted into the intra-mesh network with high rate), we can claim that dynamic allocation of subcarriers should target decrease of latencies in transmission side queues. Therefore, in this project, we seek subcarrier arbitration mechanisms which tries to minimize the transmission side delays and buffer capacities. The details of protocol specifications for on-chip packets, such as detailed definition of packet bits etc., are out of scope of this thesis work. A generic type of bandwidth allocation mechanism is envisaged, which can be utilized for any type of massive multicore architecture. 

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.92\textwidth, height= 0.55\textwidth]{./Chapter4_Figures/Chapter4_01.pdf}
    \rule{35em}{0.5pt}
  \caption[Bandwidth allocation problem in WiNoCoD can be formulated as arbitrating 1024 subcarriers to 32 transmission queues of tilesets on different symbols.]{Bandwidth allocation problem in WiNoCoD can be formulated as arbitrating 1024 subcarriers to 32 transmission queues of tilesets on different symbols.} 
  \label{fig:Electron}
\end{figure}

Based on changing parameters such as queue lengths of on different instants, the allocation algorithm may aim to minimize the average latency, a delay or buffer length exceeding probability or a totally different cost function. Next, we revise a vast literature on the similar problems of bandwidth allocation to parallel queues which is encountered in very diverse fields such as space telecommunications, processor scheduling, optical communications, cellular communications etc. 

\subsection{Cognitive Radio}

\textit{Cognitive radio} \cite{mitola2000cognitive} is all about making a radio device which is self-adaptive to the changes in its environment, in order to improve; or optimize if possible, its radio capabilities \cite{palicot2013radio}. For this purpose, it requires 3 functionalities : 


\begin{itemize}
\item Sensors : to capture changes in its radio environment, which can be the absence of transmission on certain part of spectrum (i.e. white space), detection of a certain type pre-coded signal (e.g. a beacon signal), detection of a change in energy levels or modulation of signals etc. 
\item Decision Making :  The required intelligent and robust algorithms that perform these optimization processes based on environmental stimuli. This can be change of utilized spectrum, modulation type, transmission power etc.
\item Adapt : Based on the taken decisions, cognitive radio equipments can change their RF characteristics. As a digitally implemented modulation, OFDM provides a great flexibility for this adaptation, such as changing utilized bandwidth etc..   
\end{itemize}

These 3 main steps of cognitive radio are continously implemented, such that after adapting its RF properties, the intelligent node continues to sensing its environment as illustrated in Fig. 4.2.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.60\textwidth]{./Chapter4_Figures/CR.pdf}
    \rule{35em}{0.5pt}
  \caption[Cognitive radio cycle, illustrating the circular flow of 3 main steps.]{Cognitive radio cycle, illustrating the circular flow of 3 main steps \cite{palicot2013radio}.} 
  \label{fig:Electron}
\end{figure}

In this work, from a certain perspective, we build an on-chip cognitive radio system, which takes into account the instantaneous transmission queue lengths of tileset RF transceivers (sensing by acquiring this information from other tilesets), run the algorithms proposed in explained in Chapter 5, 6 and 7 to choose the required bandwidth (number of subcarriers) and modulation order (decision step) and apply these to its OFDMA based transceiver (adaption step).

\subsection{LTE}

As OFDMA is an ubiquitous modulation for wired and wireless standards, one may profit by examining the existing solutions developed for these systems. Especially LTE 4G is the recent cellular standard which utilizes OFDMA in downlink for its medium access scheme \cite{zyren2007overview}. Existing literature for partition of bandwidth in spatial and temporal dimension among mobile nodes, rate allocation mechanisms, coordination between the base station and mobile nodes, carrier aggregation etc. can form a basis for current and future conception of WiNoCoD and similar projects. Even though cellular systems have a much lower bandwidth and less strict timing requirements, certain ideas from this domain can be migrated to our problem. For instance, Fig. 4.3 shows the carrier aggregation concept, where non-contiguous bands of spectrum can be aggregated by a node to increase data rate. In a sense, this concept is also applied in WiNoCoD thanks to the OFDMA based RF infrastructure. 


\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.80\textwidth]{./Chapter4_Figures/carrierAggregation_v2.pdf}
    \rule{35em}{0.5pt}
  \caption[Carrier aggregation concept which is also used in LTE, where non-contiguous and different widths of bands can be aggregated to be used by a node.]{Carrier aggregation concept which is also used in LTE, where non-contiguous and different widths of bands can be aggregated to be used by a node.} 
  \label{fig:Electron}
\end{figure}


\section{Dynamic Bandwidth Scheduling for Parallel Queues}

Aforementioned problem is referred as ``Multi-user Multi-server" or ``Parallel Queues" Scheduling in the literature. Countless number of varieties of this problem are encountered in different areas such as; multiprocessor scheduling (computational scheduling) \cite{shivaratri1992load}, satellite communications \cite{ganti2002tranmission}, optical networks \cite{ji2013design} etc.. When formulating the optimal scheduling policy, generally the average delay (or identically average queue length from Little's Law \cite{little2008little}) is chosen as the function to minimize \cite{kunz1991influence}, which is also the main metric of interest for on-chip networks.  

Research on scheduling policies for multi-user multi-server case incorporates fields of Queuing Theory, Network Theory, Stochastic Optimization and Markov Decision Processes. Considering the nature of our OFDMA interconnect, we will investigate the most effective algorithms which are referred as dynamic scheduling policies. These type of policies reassign servers to the queues every single or multiple time slot(s), based on the instantaneous state of the system. Even though certain state metrics such as recently measured mean response time or mean queue lengths, or utilization of the servers can be used to determine the new server allocation, generally algorithms who are using instantaneous queue length state information (QSI) are best (queue aware policies) \cite{kunz1991influence}. 

Even though this problem might appear as straight-forward, optimal solution generally requires complex stochastic operations and only valid for very limited circumstances with specific unrealistic assumptions. In this section, we present some of the approaches for dynamic bandwidth scheduling, which are most convenient for WiNoCoD case. 

\subsection{Longest Queue First}

For the case where there is a single server, which has to be assigned to one of \textbf{K} queues every time slot (i.e. multi-user single-server case), it was proven that Longest Queue First (LQF) policy provides the lowest average latency, given that arrival processes to queues are independently and identically distributed (i.i.d.). This policy simply assigns the server to the queue who has most number of packets (i.e. longest queue). Although it is straight-forward, this policy has received a wide attention from research community for its stochastic analysis under certain circumstances and variables. \cite{jagannathan2013impact} compares the buffer flow exceeding probabilities of LQF with any other non-queue aware policy, and also proves that it is also the optimal policy in terms of buffer exponents given that arrivals are i.i.d..

The optimality of this algorithm for multiple server case is still valid under i.i.d. arrivals assumption. \cite{kittipiyakul2009delay} investigates the multi-queue multi-server problem in the context of wireless communications, where connections between users (queues) and certain frequency channels (servers) are not available for certain instances. This stochastic assumption for frequency selective channels in wireless communications is common. They prove that LQF among the connected servers is the optimal solution under i.i.d. arrivals assumption. Explaining in detail, the suggested algorithm (Longest Connected Queue First) iterates through all servers, and assigns each server to the longest queue among users who are connected to it on that time slot. It is trivial to see that this optimality is valid for the special case of full connectivity, where all queues are connected to all servers always. Note that, this is the case for WiNoCoD's OFDMA RF interconnect, where each tileset can use any of the 1024 subcarriers. However, the literature generally approached this queuing problem from the wireless communications point of view, where channel conditions are considered and a joint power/rate scheduling is performed. Their algorithms generally use complex maximization via utility functions \cite{li2010dynamic}\cite{eryilmaz2005fair}. Due to static nature of channel and transmission dynamics, our problem in WiNoCoD is purely a queuing optimization. 

As stated previously, this algorithm is optimal in the sense of minimizing average latency for the special case of symmetric queues, where stochastic arrivals to queues are i.i.d. distributed. However, for most of the time this assumption is far to be accurate. Especially for the on-chip traffic, as we have revised in Section 2.4. In addition, this algorithm may cause a severe starvation, as certain nodes with high queue lengths may starve all the resources (servers), and packets in small length queues may not acquire any resource for a long period. Another drawback of this algorithm is its computational complexity. It iterates through each $N$ servers, and each iterated server iterates all $K$ queues. And after the assignment, the queue lengths are updated. Therefore it has a complexity order of $\orderof\left( KN\right)$, which increases with the number of queues and servers. Execution of this algorithm may be feasible for certain scenarios such as cellular communications where bandwidth is reallocated every few hundred miliseconds, however time required for computation in an on-chip context should be limited to nanoseconds. 


\subsection{Queue Length Proportional Scheduling}


Another solution to multi-queue multi-server allocation problem is to divide the total bandwidth proportional to instantaneous queue lengths. This approach is known as ``Queue Proportional Scheduling"(QPS) or ``Buffer Length Proportional Rate" in the literature \cite{dovrolis1999proportional}. Even though its delay optimality has not been proven, it provides a very good compromise between fairness and low average queuing delay. It was shown that this algorithm is capable of providing a proportional average delay differentiation under high traffic load \cite{dovrolis1999proportional}. This means, the average delay of two different queues can be guaranteed to have a desired arbitrary proportion as $\frac{d_{i}}{d_{j}} = \frac{c_{i}}{c_{j}} $, where $c_{i}$ and $c_{j}$ are two adjusted scalars, which are multiplied by instantaneous queue lengths, while proportionally dividing total bandwidth. This may be a strong tool for the scenarios where a Quality-of-Service (QoS) differentiation is required. In our case, all tileset queues have the same priority, i.e. $c_{i} = 1$ for all tilesets. Hence, it can be deduced that under high input traffic close to the system capacity, this scheduling would yield to an equal average latency at all tileset transmission queues. 


\subsection{Square Root of Queue Length Proportional Scheduling}

Complexity of stochastic optimization without subtle assumptions, forces researchers to approach the multi-queue multi-server problem via different methods. Especially, without well defined arrival process distributions, queuing theory equations are hard to derive. For instance, \cite{ngin1999generalised} approached this problem from a different perspective. The authors intend to design a rate scheduler for classical ATM networks. The total service rate (can be visualized bandwidth) of $R$ should be divided among $K$ ATM nodes, i.e. queues as $r_{i}$. They formulate this problem as dividing the total $R$ to $K$ queues such that, it minimizes the time to drain all the backlog in $K$ queues. In this case, they assume no further packets arrive to the queues, so the optimal arbitration clears out all the packets in the system as quick as possible. Writing the problem formally :

\begin{align}
arg min  \left\{ \frac{Q_{1}}{r_{1}} +  ... + \frac{Q_{K}}{r_{K}} \right\} \, \, s.t., \, \sum\limits_{i=1}^K r_{i} = R
\end{align} 

They also study the case for proportional service differentiation as we have mentioned previously, where the total service acquired by two arbitrary nodes are proportional to a constant. However, we investigate the case for no service differentiation, where all queues have the same priority. After using dynamic programming (DP) techniques, they reach an elegant analytic solution, where the optimal rate allocation is proportional to square roots of the queues. The optimal rate should be allocated to two arbitrary nodes has a relation to their current queue lengths as follows :

\begin{align}
\frac{r_{i}}{r_{j}} = \frac{\sqrt{Q_{i}}}{\sqrt{Q_{j}}} 
\end{align} 

However, recall that this assumption optimizes the queue draining times and assume no further exogenous packet arrivals. Thus, this approach may be unsuitable for certain cases. 

\subsection{Oldest Packet First Scheduling}

Apart from scheduling algorithms using instantaneous queue length information of the queues, we would like to introduce another approach. This scheduling is omniscient in sense of knowing current delay of each packet in each queue, which is a non-realistic assumption for most of the scenarios. At every scheduling instance, the proposed algorithm allocates $N$ resources to oldest $N$ packets. If scheduling is performed every time slot, it is straight-forward that this algorithm yields to the minimum average and maximum delay. It is generally referred as \textquotedblleft Oldest Packet First\textquotedblright (OPF) algorithm in the literature \cite{homan2002enhanced}. This algorithm not only needs to know each delay of each packet in the system, but also requires high number of iterations to choose the oldest head-of-line (HoL) packet in each queue for each of the resource to allocate. Even though this scheduling may seem unrealistic to apply in our case, we will use this algorithm as an optimal reference to compare our proposed schedulers.

\section{Preliminaries of Bandwidth Allocation in WiNoCoD}

\subsection{Partitioning Bandwidth Statically}

A basic instinctual approach for allocation of subcarriers for transmission would be to divide them equally among tilesets. ATAC \cite{kurian2010atac}, as mentioned in Chapter 2, is a thousand-core hybrid cache coherent CMP architecture comparable to WiNoCoD but utilizes an optical interconnect. Similarly to WiNoCoD, for dimensionality purposes cores are clustered and there exists 16 clusters, each containing 64 identical cores which can access the optical waveguide. As explained in detail in Chapter 2, each cluster can transmit their information on 128-bit wide channels, which is composed of a different wavelengths (WDMA) on different separated parts of the waveguide. In reception part, all clusters are tuned to whole dedicated bandwidth, that they can receive all packets transmitted by others. This is similar to intrinsic broadcast capability of WiNoCoD's OFDMA interconnect, where main motivation behind this SWMR approach is to support large amount of broadcast cache-coherence packets of distributed hybrid memory. Recalling Chapter 2, this bandwidth intense cache coherence protocol and enabling SWMR interconnect, helps to isolate programmer from the hardware, where scalable, easy programmation can be done in thousand-core architectures. Due to static nature of these optical channels where they are generated by non-tunable circuitry, the researchers were obliged to allocate a bandwidth portion statically to tilesets at TX. They have chosen to equally partition the optical bandwidth to all 64 tilesets as in Fig. 4.4(a). 

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.99\textwidth]{./Chapter4_Figures/staticAlloc3.pdf}
    \rule{35em}{0.5pt}
  \caption[Static and equal or non-equal allocation of frequency resources among multiple nodes.]{Static and equal or non-equal allocation of frequency resources among multiple nodes.} 
  \label{fig:FlitsAndRB}
\end{figure}

The fundamental assumption behind this choice is that with non-uniform memory architecture, with broadcast intense hybrid distributed cache coherence protocol will lead to a uniform demand from tilesets on average. First of all, this assumption depends on the chosen application, where we know they may be so diverse in terms of spatial locality \cite{soteriou2006statistical}. A more significant point is the instantaneous changing demands, that we know cores produce highly different amount of cache coherence packets both in small intervals of few cycles or much longer application phases \cite{nychis2012chip}. Hence, designers of this optical interconnect were obliged to dimension their photonic bandwidth concerning the maximum load that a tileset generate. It is considerable to think this mechanism not as scalable and bandwidth efficient, even when the average load of sharing nodes are equal, as generated number of packets by a tileset is a sporadic phenomenon and varies significantly over time. 

The projection of this approach on WiNoCoD, would be to allocate equal number of subcarriers on each slot for each tileset. Considering we have 1024 subcarriers and 32 subcarriers, we could allocate 32 subcarriers per tileset per symbol for transmission. In case QPSK used, this corresponds to a configuration where each tileset can transmit 64 bits per 50 ns, thus a 1.25 Gbps rate, constantly and equally allocated for each tileset. Of course, this choice would require no reconfiguration mechanism or overhead but would be far away to exploit the highly efficient and rapid reconfigurability of OFDMA. Arbitrating bandwidth to nodes based on their instantaneous demands is a strong tool to achieve much lower delays and buffer sizes, but conventional optical or RF interconnects are limited by their static circuitry, where OFDMA alter this situation.

\subsection{A Quasi-Static and Quasi-Dynamic Modification}

Another perspective to partition the bandwidth is to be able to distribute it to nodes per used application. Certain applications may demand much less computational power and memory. Taking into account our 2048 cores and 1 TByte RAM, certain applications can be executed using much smaller parts of CMP. For instance, in case 4 out of 32 tilesets are guaranteed to be used by the application, whole RF bandwidth could be distributed for transmission of 4 tilesets only through the execution, which effectively increases available bandwidth by 8. In ATAC, where transmission channels are generated by non-tunable optical rings, we can claim even an application based quasi-static bandwidth arbitration cannot be effectuated. 

In addition, even all applications are used, the total bandwidth demand of each tileset would not be equal. This is mainly due to fractal locality of computation and memory access patterns, which has received a wide attention from the research community. In \cite{greenfield2010rentian}, it was shown that this locality of on-chip traffic (that becomes much more apparent as core number reaches thousands) can be formulated accurately by \textit{Rent's Law} which was actually developed to model the mathematical relation between number of output terminals and number of components in an integrated circuit. This theory, basically states that the traffic intensity between nearer cores is exponentially higher compared to farther cores. Also in \cite{rahmani2009negative} the communication probability of a core to an another core was tried to modeled by a negative exponential distribution. \cite{bahrebar2014characterizing} attempts to characterize this locality especially for a 3D NoC interconnect. Authors in \cite{manevich2012handling} and \cite{more2013network} proposes NoC architectures to exploit this locality. Finally, \cite{soteriou2006statistical} analyzes various number of diverse application run on a CMP and a mesh NoC, where they find out that total throughput generated by a core through whole execution time is distributed based on a Gaussian distribution. Briefly, this means for certain applications, even whole CMP cores and memory would be utilized, certain tilesets may use exponentially higher bandwidth compared to other ones. 

Considering much more complex schemes, where multiple different characteristic applications are embedded on CMP, the bandwidth demand  difference of tilesets shall be much more diverse. In a nutshell, if the demands of tilesets for certain applications and configurations through the execution time can be foreseen at compilation time, different number of subcarriers can be allocated to appropriate tilesets easily, thanks to basic digital reconfigurability of our OFDMA based interconnect as in Fig. 4.4(b). Even this configuration scheme does not exploit the temporal changes of traffic, this quasi-static modification with no overhead and effort at run-time, shall provide much higher performance compared to a fully static architecture like ATAC, where bandwidth allocated to tilesets are always same and equal. Undoubtedly, using rapid, trivial and efficient bandwidth arbitration mechanisms, our OFDMA interconnect can provide much more performance using temporal locality as an additional dimension.

Another possible option OFDMA provides us the offline optimization of the bandwidth allocation for execution of certain applications, especially non-real time ones. Actually, we can draw a straight analogy between subcarrier arbitration for OFDMA on-chip interconnect and multiple CPU resource allocation for applications. They both seek the optimal allocation of a resource (first one being data rate, and the latter one being computational resource) to reduce latencies. Especially, there exists a vast literature on the allocation of multiple cores to workload in thread or instruction level \cite{frachtenberg2009job}. 

Like for the offline multi-core job scheduling analysis \cite{pricopi2014task}, optimal number of subcarriers and chosen modulation order for each tileset on each symbol can be determined. This shall boost the communication performance of the architecture, decreasing latencies to minimum. The previously presented Oldest Packet First (OPF) algorithm may be a candidate for this kind of optimization. However, in this thesis work we deal with the on-time dynamic allocation of resources, which is a more generic approach for all kind of applications. 

\subsection{Resource Blocks, Frames and QSI Signaling}

Peculiar limitations of WiNoCoD's OFDMA RF interconnect require to develop new techniques to cope with. As mentioned previously, the relatively long symbol duration with respect to inter-packet arrival times, short packet lengths, and extremely strict delay requirements make this environment really unique compared to any existing OFDMA based communication medium. Therefore, in this section we introduce certain preliminary notions rooted from these limitations, for bandwidth allocation in WiNoCoD. This context shape a new allocation problem concept, where we have to derive new solutions, as extensions of the ones presented here.     

\subsubsection{Resource Blocks}

First notion we present is the \textit{Resource Block} (RB), which defines a group of adjacent subcarriers on a single symbol. This term is borrowed from LTE jargon, where it is used for a group of adjacent subcarriers spanning multiple symbols in time \cite{sesia2009lte}. Considering we have 1024 subcarriers, it is obvious that a very fine granularity for bandwidth allocation with one or few subcarriers is both computationally challenging and unnecessary. As symbol duration is relatively long (consider the case cores are operating at 1 GHz frequency, resulting in a symbol duration more than 50 cycles which is a remarkable time interval in terms of processing), we can state that a bandwidth reconfiguration which is performed every symbol would be an essential requirement. We choose to define a RB to serve exactly 1 short packet (1 flit - \textit{64 bits}). This way, a short packet (which constitutes most of the packets circulating on RF interconnect as mentioned in Section 2.1) can be served in one symbol, regarding the unique long duration of OFDM symbols. Consider the case we define a RB of 16 subcarriers, where a flit is 64 bits. Assuming QPSK is used, 1 RB corresponds to 32 bits. Taking the example for a tileset which is allocated 5 RBs (80 bits) and it has 1 flit short packet of 64 bits in its queue. Therefore, it would utilize only 64 subcarriers and left idle the remaining 16 subcarriers, which is a waste of resources. For these reasons above, a RB is defined as 64 bits (integer multiple of a packet) of service in 1 symbol (32 subcarriers in case of QPSK, and 64 subcarriers in case of BPSK). Based on the progress of the project, flit size-packet fragmentation shall be regulated, however to test our algorithms, QPSK has been chosen as the default utilized modulation order and 64-bit flits, thus and a RB of 32 subcarriers is assumed, allowing each tileset to have a chance to send 1 flit per symbol. Symbol-wise on-chip packet scheduling with RBs is shown in Fig. 4.5.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=1.00\textwidth, height= 0.6\textwidth]{./Figures/Flit_transmission_and_RB_.pdf}
    \rule{35em}{0.5pt}
  \caption[]{A Resource Block allows for the transmission of 1 flit during a 50 ns symbol in WiNoCoD} 
  \label{fig:FlitsAndRB}
\end{figure}



\subsubsection{Frames}

After we partition our 20 GHz spectrum as RBs (in case of QPSK, 32 RBs per symbol), now we have to define the \textit{structure of allocation}. As we have reviewed in Section 4.2, effective rate allocation algorithms require to utilize the recent queue length information (QSI) of queues, in order to attain low delays and buffer overflow probability. However, taking into account the limited bandwidth per symbol, exchange of QSIs of each tileset every symbol seems not realistic due to overwhelming signaling. This cost of QSI signaling overhead will be examined in Section 4.3.3.6. In addition, most important requirement to perform re-arbitration of subcarriers is to give the microprocessors or RF controller modules in tilesets (or central intelligent unit) the time for computation. In conventional OFDMA networks, for instance LTE, computation time for allocation algorithms is not a bottleneck. 1 symbol takes around 66 microseconds in LTE, and due to reasons like channel coherence duration and propagation time (RBs are re-allocated every 13-14 symbols), computation for allocation is assumed to be performed instantly between symbols. However, in our specific case in WiNoCoD, where 1 symbol is 50 ns, the computation time appears as a significant actor, as the temporal limits of semiconductor technology is reached at the scale of nanoseconds. This strict and unorthodox temporal requirement makes the allocation problem which is examined in this thesis an original work compared to conventional networks. Not just the computation time, but all the cumulative delay incurred by subcarrier reconfiguration time, packet processing, synchronization, propagation time etc. can be treated in one frame. A generic microprocessor can be employed for bandwidth allocation, whereas a dedicated digital circuitry may be implemented in order to decrease computation time further. Also, we can expect the scale of 20 GHz OFDM bandwidth to hundreds of GHz in several decades, shortening symbol duration more and more, with rapidly developing semi-conductor technology. In case, economic, low-scale generic processors will be continued to be employed for bandwidth reconfiguration, this bottleneck of computation time shall continue to tighten. As mentioned previously, IFFT/FFT computation with effective state-of-the-art modules and synchronization latency between tilesets due to propagation time are assumed to be zero (or included in symbol time), however in case these aspects are decided to be treated, this \textit{computation time} can be assumed to include them, where queue length information becomes outdated. Hence, we envision a system, where QSIs (or other indicators of local traffic taking into account other possible algorithms) are broadcasted by tilesets every \textit{T symbols} and the new allocation of subcarriers are activated after $T$ symbols. Thanks to broadcast capable OFDMA interconnect, the QSI information sent by each tileset can be received by others. We call this $T$ symbols as a \textit{frame}. Thus, at the end, we have a pipelined allocation system, where allocation at the start of a frame is calculated according to QSI information sent $T$ symbols before. Of course, we need certain number of subcarriers to be reserved on first symbol of a frame, in order to allow tilesets to encode their QSI. Fig. 4.6 illustrates the structures of frames and RBs in WiNoCoD. This time-frequency matrix composed by atomic temporal units of \textit{symbols} and spectral units of \textit{RBs} constitutes the backbone of the bandwidth allocation mechanism in WiNoCoD. We provision that tilesets have a certain digital circuitry and microprocessing system that implements this time-frequency (RB-symbol) matrix, and they modify it at the start of every frame, where each RB corresponds to a tileset ID. At every symbol, they use this virtual matrix to encode their flits before IFFT phase, on the subcarriers reserved to them (if there is any on that symbol). And for the receiver side, they do the necessary packet treatment and routing on received flits on each RB, with the ID associated with it. In other words, tilesets implement this imaginary matrix notion in their RF controllers, therefore they know on which symbol and on which RB, they shall transmit their flits and receive flits of other tilesets associated with their IDs. Note that, in case a central allocation paradigm is adopted, where a central intelligent unit (CIU) gathers local QSIs, deciding on the new configuration of this matrix must signal other tilesets about the bandwidth partition with a response message. Hence, additional subcarriers should be reserved at the end of the frame.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=	0.85\textwidth]{./Chapter4_Figures/Chapter4_03.pdf}
    \rule{35em}{0.5pt}
  \caption[Frames and Resource Blocks (RBs) in WiNoCoD]{Frames and Resource Blocks (RBs) in WiNoCoD} 
  \label{fig:Electron}
\end{figure}

It is important to note that, the temporal delay mentioned here for allocation procedure, does not only includes the computation for arbitration but also includes all delays incurred by wave propagation, IFFT/FFT, serialization/parallelization, conversion and synchronization, both in transmitter and receiver sides. Therefore, in extremely rapid changing bandwidth demands of this specific configuration of WiNoCoD, with bimodal packets and different number of packets created every cycle by different tilesets, we have chosen to build this pipelined framework, where outdated QSI cannot be omitted as in conventional networks. Recall that one symbol duration is approximately 50 ns, which corresponds to 50 clock cycles for processors running on 1 GHz. Rather than strictly defining these delays explicitly and individually, we have chosen to set a cumulative delay in terms of OFDM symbols, firstly because project is still in progress where these values are not certain yet, and also to offer a generic comprehension of the allocation algorithms, that their performances are evaluated under different frame lengths. Moreover, one can also determine different computation times based on chosen allocation algorithms or used microprocessor(s) with different speeds or ASIC implementations with different trade-offs between execution speed and power consumption, surface or budget. However, chosen frame lengths in experimental evaluation are all feasible and validated by the primal designs of the architecture.  	





\subsubsection{Decentralized and Centralized Allocation}

The subcarrier arbitration mechanism we have developed for WiNoCoD interconnect, can be classified into two cases, such as decentralized and centralized approaches. Both approaches have pros and cons. If a centralized allocation is preferred, a single central intelligent unit (CIU), which either can be a digital circuitry or simple microprocessor(s) installed inside a tileset's RF front-end (Before IFFT/FFT module). This way, it can use the OFDM modulator/demodulator of that tileset. It shall acquire the QSIs of each tileset, process the necessary allocation algorithm, determine the arbitration of RBs for next frame and send to each tileset its allocation scheme. Thanks to this full broadcast capable structure, we do not need any beacon signal or similar channels like for most of the wireless cognitive radio or ad-hoc networks. To lower the transmission power requirement and error probability, CIU shall be installed in a tileset which is physically close to the center of transmission line. This is an obvious extra signaling overhead, which is not desirable for our limited on-chip bandwidth. Another disadvantage is the robustness. In case, a circuit failure happens, the bandwidth allocation infra-structure of the system collapses. In addition, with request-response nature of allocation, the response time of the system is increased. The main aim in central allocation is to minimize the intelligence at tileset front-ends, thus lowering the area or cost budget. The exact signaling of which tileset will use which RB is impossible due to extensive overhead, thus CIU shall broadcast only the \textit{number of RBs allocated for each tileset}, and a simple circuitry in each tileset should map these to its \textit{allocation matrix}, by sequentially placing them starting from the first RB. For instance, indicating a tileset to utilize RBs $R_{1}, R_{2}..$,  on symbol $t_{1}$ and RBs $R_{3}, R_{4}..$ on symbol $t_{2}$ .. is overwhelmingly bandwidth consuming, which is not feasible in our case. Hence, central unit shall only indicate the number of RBs allocated for a tileset in next frame. 



\begin{figure}[htbp]
  \centering
    \includegraphics[width=1.00\textwidth, height= 0.6\textwidth]{./Chapter4_Figures/FrameStructureCentral_v2.pdf}
    \rule{35em}{0.5pt}
  \caption[Central Unit Bandwidth Allocation]{Allocation of RBs through a frame, in central bandwidth allocation} 
  \label{fig:Central Unit Bandwidth Allocation}
\end{figure}


Also, note that we have positioned the response of CIU on reserved subcarriers on few symbols before the end of the frame as in Fig. 4.7. We spare $T_{reconfiguration}$ symbols of time for tilesets' processors to receive the number of RBs allocated for each tileset in next frame and compute the RB allocation pattern and reconfigure their transmissions. 
								
\subsubsection{Direction of Resource Block Allocation in a Frame}

After marking the headlines of the frame structure and the pipelined fashion of RB allocation using recent QSI information, an important question arises : in which direction these RBs should be allocated? In other words, when we start to allocate RBs from the first RB in the frame, which one should be the next? This is a significant and thought-worthy aspect, both considering the network layer due to variation of delays and physical layer due to the fact that transmission power changes based on using adjacent subcarriers or not. We propose 2 different approaches as illustrated in Fig. 4.8 : Allocating RBs in \textit{frequency direction} and \textit{time direction}. We may claim that allocating RBs in frequency direction starting from the requirements of a tileset would allow it to clear out its queue in shortest duration as possible, however this would be unfair for the rest of the tilesets. Especially, in case of long frame lengths, allocating all RBs for a tileset on just first few symbols is disadvantageous, as this tileset would not have any RBs left in the rest of the frame taking into account new packets shall arrive during the current frame (Consider a relatively long frame, that new packets shall continue to arrive each symbol during the frame). These newly arriving packets would require bandwidth also, and may disrupt the latency performance dramatically. This situation may dramatically increase the variance of packet service duration, which increases the overall latency as we know from queuing theory. If we allocate RBs in explained frequency direction, a group of adjacent RBs may be concentrated along one or few symbols. In contrast, allocating RBs in time direction would increase the chance of a tileset to have at least one RB on each symbol of a frame). Thus, allocating RBs in time direction appears as another solution, which seems to allocate RBs more uniform in temporal manner.  Therefore, as the performance of two proposed approaches cannot be estimated analytically, we both test them under various scenarios and configurations. 

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.80\textwidth]{./Chapter4_Figures/Chapter4_05.pdf}
    \rule{35em}{0.5pt}
  \caption[Allocation of RBs through different directions inside a frame]{Allocation of RBs through different directions inside a frame} 
  \label{fig:Allocation of RBs through different directions inside a frame}
\end{figure}

\subsubsection{Idle Resource Blocks-Default Frame Configuration}

In particular instants, sum of RB demands of tilesets (i.e. total QSI) may be much lower than the total number of RBs in a frame. For these cases, there may exist RBs left idle. Considering also the potential new packets which arrives during these \textit{idle symbols}, where all or certain RBs are not used by any tileset-, sharing them equally among tilesets seems straightforward. In fact digital nature of OFDMA allocation makes this process quite trivial. We envision a virtual \textit{default allocation matrix} where RBs are arbitrated in an FDMA fashion which allows for a RB to be utilized during each symbol for each tileset. In other words, in a default allocation matrix, each 32 tilesets have 1 RB each. Especially in case of time direction allocation, if we would keep the same FDMA configuration, this would impose an unfairness on the tilesets which utilize the RBs on the upper side of the frame (where RB allocation starts). Therefore, we decided to employ a basic rotating mechanism for this default configuration as in Fig. 4.9.

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.90\textwidth]{./Chapter4_Figures/Chapter4_06.pdf}
    \rule{35em}{0.5pt}
  \caption[Default allocation matrix]{Default allocation matrix which is evenly distributed among tilesets in an FDMA fashion, is modified by the scheduler based on the reported QSIs. The rest of the RBs which are left idle, is partitioned according to this uniform distribution} 
  \label{fig:Default allocation matrix}
\end{figure}

\subsubsection{QSI Encoding and Signaling}

An important aspect is the encoding of QSIs of tilesets at the start of a frame. In case of QPSK constellation, we have 2048 bits available on a symbol. Based on our extensive simulation campaign, 8 bits per tileset per frame to encode QSI has shown the best overall performance. With 8 bits, it is possible to encode 256 different levels. Of course, the empty queue (0 QSI) should also be indicated. By looking at simulation results, we have seen that the number of flits in a transmission queue very rarely exceeds 255 under evaluated scenarios. Therefore any QSI between 0-255 is directly encoded, and if the number of flits is larger than 255 it is encoded as 255. Of course, one can choose different approaches and granularity of QSI encoding, such as representing multiple flits by a single QSI level. There is an evident trade-off between QSI signaling overhead and accuracy of the allocation algorithm. However, from these many possibilities, by taking account the realistic interconnect configurations, proposed 8-bit encoding seems viable and effective. 


On first symbol of each frame, tilesets have to signal their instantaneous QSIs after encoding the number of flits. Thanks to intrinsic broadcast capability of OFDMA, each other tileset or -\textit{CIU in centralized case}-, can receive all of the QSIs. For this purpose, certain number of RBs on first subcarriers, on first symbol is reserved, as default. For instance, consider the example, 1 RB is composed of 32 subcarriers (QPSK-64 bits) and QSI for each tileset is encoded with 6 bits. Then, we need 32x6 = 192 bits (subcarriers), which corresponds to 3 RBs in this specific case. One can see that, total number of subcarriers for QSI encoding of 32 tilesets, should be an integer multiple of RBs, in order to avoid waste of resources and invalidation of the RB based allocation mechanism. In case, the QSI encoding would be done with 7 bits, with default QPSK mechanism, we would need to allocate either 3 or 4 RBs, which corresponds to 6 or 8 bits of encoding per tileset.   

Another important aspect of QSI encoding and signaling is the robustness counter transmission errors. QSI sent by a tileset may be received erroneously by different tilesets, resulting in a discoordination of bandwidth allocation, causing collision at the end. We assume, these kind of errors are out of scope of this thesis, however one can take precautions to decrease probability of errors on QSI channels further, by employing BPSK, the lowest modulation order or a mechanism to allocate more power on these signaling subcarriers.




\subsubsection{Taking Into Account the Outdated QSI}

Very specific constraints of on-chip OFDMA interconnect enforces certain requirements. Considering the relatively long symbol duration compared to packet delay requirements, even a latency of few number of symbols is important. In this special context, just in few symbols, whole queue dynamics may change drastically. As explained above, due to constraint of computation time and signaling overhead, the allocation is done in a pipelined manner through frames. In other words, the allocation is done with the $T$ symbols outdated QSI. Hence, when the new reconfiguration of the bandwidth is activated, the QSI values may have changed significantly. For instance, consider the example where a tileset's demand is 8 RBs, and it has been already allocated 12 RBs during current frame and no new arrivals has occured. As the allocation is done for QSI of 8, it would be allocated unnecessary amount of resources, while it has no packets in its queue at the start of the new frame. Considering the highly temporally heterogeneous on-chip cache coherency traffic, we introduce the notion of \textit{Definitive QSI} (DQSI). At the beginning of each frame, before encoding its QSI on the subcarriers, each tileset already knows the allocated number of RBs in the current frame. Therefore, rather than encoding QSI directly, it subtracts newly allocated number of RBs ($S_{i}^{t}$) from its QSI ($Q_{i}^{t}$). Of course, we eliminate the negative QSI, equating it to 0 for the negative values.  : 

\begin{align}
Q_{i}^{t} = max(0, Q_{i}^{t} - S_{i}^{t})
\end{align} 

This operation is fairly simple, negligible in computation time $T$ and processing power, but has a great potential to avoid unnecessary bandwidth allocation. Especially, when the QSIs are small, this value will tend to be zero, approaching to a equal allocation of RBs. In literature, various approaches like this to eliminate unnecessary bandwidth allocation due to outdated QSI can be found in certain protocols \cite{zhu2014cross}\cite{ansel2006fhcf}. Once the \textit{minimum number of flits} in the queue is determined, we may also take into account the number of flits that will arrive during allocation process (i.e. current frame). To do so, the tilesets has to estimate the number of flits that will arrive during a frame. Considering the self-similar temporal nature of on-chip traffic, these estimations shall be accurate enough. We refer this as the \textit{Expected QSI}(EQSI), which is calculated as :

\begin{align}
\hat{Q}_{i}^{t} = max(0, Q_{i}^{t-1} - S_{i}^{t-1}) + A_{t} 
\end{align} 

where $A_{t}$ is the estimated number of flits that will arrive during a frame. For estimation an \textit{Exponentially Weighted Moving Average} (EWMA) filter can be used, which calculates the moving average as follows :

\begin{align}
A_{t} = \alpha A_{t-1} + (1 - \alpha)a_{t-1}
\end{align} 

$\alpha$ is a scalar, which weighs the importance of recent observation and $a_{t-1}$ is the number of flits arrived in last frame. The optimal value of $\alpha$ obviously depends on the traffic. Via extensive simulations under various scenarios, it shall be determined. Another approach is to take the number of arrivals in last frame as the estimation, which is the extreme case of EWMA with $\alpha = 0$. Note that, $\alpha$ depends on the nature of the traffic and may be determined online or offline intelligent algorithms, however this is out of scope of this work. Based on our simulations, we have determined and tested an $\alpha$ value of 0.95 in this thesis.  The resulting bandwidth demand is labeled as \textit{Expected QSI}(EQSI). Even though it is quite different, \cite{rizk2015queue} also takes into account the average arrival rates for allocating bandwidth in epochs.  

For decentralized and centralized approaches, calculation mechanisms for DQSI and EQSI differs. Firstly for the decentralized case, each tileset computes its own DQSI or EQSI, and signals this value for the allocation process to other tilesets. This eliminates the redundancy and greatly reduces the computational costs. Therefore, the QSI is encoded just after the calculation of the number of RBs in next frame and few cycles before the end of the current frame to allow the computation of DQSI or EQSI. Each tileset governs an EWMA for estimating its own EQSI. At each frame, tileset front-end is responsible of counting the number of flits that has arrived in last frame and computation of the moving average.

In contrary, for centralized approach, in order to minimize the computational burden at tileset front-ends, CIU is responsible for DQSI and EQSI calculation. Tilesets simply signal their QSIs and CIU uses this information to calculate DQSI or EQSI for each tileset before allocating RBs based on these values. For this purpose, CIU keeps the number of allocated RBs for each tileset in last frame. CIU governs 32 EWMA registers for each of the tilesets. As CIU cannot know the exact number of flits that has arrived in a frame to each tileset, it estimates this value for each tileset by using the current QSI value, last QSI value and allocated RBs in last frame :

\begin{align}
A_{t-1}^{i} = Q_{t-1}^{i}  - Q_{t}^{i}  + S_{t-1}^{i} 
\end{align} 


The computational cost of EQSI or DQSI for CIU is roughly more than 32 times for the tilesets in decentralized mode. Note that these operations are parallel in nature, so they can be exploited by multiple processors employed in CIU.  




\subsection{Traffic Models and Evaluation Methods}

Before we present our algorithms, it is essential to introduce the techniques and scenarios we will utilize to evaluate them. We use OMNeT++ \cite{varga2001omnet++}, a discrete event simulator, to mimic the dynamics of 32 tileset transmission side queues, which is utilized widely among Network-on-Chip community. Simulations are run in a symbol-accurate manner, that the atomic unit of time is one OFDM symbol. This means that we approach the problem from the grounds of improving the latency performance of packets waiting to be served at RF interconnect. Therefore, any traffic/congestion issues and latencies incurred in intra-tileset network (2D electrical mesh, tile crossbars etc.) are omitted. Also, content of the packets and their priorities are ignored, where they are treated as subjects demanding bandwidth with certain quality of service. This approach is adopted in this thesis work, because isolating RF interconnect and optimizing its performance provides the possibility to employ a more generic massive CMP attached to it, opening a whole new avenue for chip designers to consider OFDMA based RF interconnect as a candidate for their specific designs. However, note that, a more holistic approach and possible extensions to this work can be developed, taking also intra-tileset traffic into account.   




\subsubsection{Metrics of Interest}

First metric of interest, that we try to improve is the \textit{average latency}. Through a deep literature review, the \textit{average latency} incurred by Network-on-Chip under different traffic models and loads, appears as the most important metric of interest \cite{lee2007chip}\cite{marculescu2009outstanding}\cite{pestana2004cost}\cite{gratz2006implementation}\cite{more2013network}. Delay between the time a cache-coherency packet is generated at a core/memory unit till the time it reaches its destination point through the Network-on-Chip is defined as its \textit{latency}, where it includes all delays caused by waiting time at buffers of routers, switching, packet processing, fragmentation and propagation time. The main motivation behind aiming to minimize average latency as a priority rather than setting strict requirements for individual packet delays, is that this metric effects application total execution time directly. However, for various real time applications with strict QoS requirements, jitter of individual packet delays may be significant \cite{vellanki2004quality} also. Most of the time, efficiency of the NoC is tested by investigating the curve showing the increasing average latency under increasing average load (i.e. known as {injection rate} in literature - average number of packets generated by processing elements at an instance). Generally, due to basics of network theory, the average latency starts to \textit{blow up} after a certain threshold of injection rate, where the system starts to be insufficient compared to generated load, where packets build-up at a faster rate than their serving at queues, hence average latency increases exponentially at a massive rate. Generally most of the papers in literature seek to increase this system blow up limit as much as possible, by their new NoC designs. Therefore, as stated previously, in this thesis, we will investigate these kind of curves, and try to increase the system limit and also decrease the average latency under this limit, as much as possible with our novel bandwidth allocation algorithms. 

We have mentioned the importance of individual packet delays in a NoC, especially for run time applications. Therefore, our RF interconnect should also be tested for this metric. For this purpose, for each simulation we evaluate the \textit{packet delay exceeding probability} curve, which shows the probability of any packet generated throughout the simulation to exceed the given delay bound, $D_{0}$, : $P(D>D_{0})$. 
 
And third metric of interest, is the \textit{buffer length exceeding probability} curve, which gives the probability for a tileset's transmission queue to exceed a certain value in terms of flits $L_{0}$, at any symbol throughout the simulation; $P(L>L_{0})$. One can see that, this metric is significant for dimensioning buffer lengths of our interconnect.

We compare the latency performance of our algorithms to a pseudo-optimal scheduler called \textquotedblleft Oldest Packet First\textquotedblright (OPF) which is explained previously in Section 4.2.4. At the start of each frame, this optimal algorithm allocates RBs one-by-one by choosing the tileset queue with the highest head-of-line (HOL) delay. As it name suggests, it always serves the oldest packet in the system first. If multiple oldest packets exists, a rotating priority mechanism is applied. In this approach, aforementioned pipelined mechanism is not used as this is a hypothetical reference algorithm. Only at each frame time a near optimal arbitration of RBs is calculated.  

\subsubsection{Employed Traffic Models}

Currently utilized application benchmarks to test NoC performance is far to represent the load generated future embedded algorithms in 1000 core architectures \cite{kurian2010atac}\cite{bienia2008parsec}\cite{woo1995splash}. There exists an intense attempt by the NoC research community to characterize the on-chip traffic and generate reliable synthetic traffic to span a wide spectrum of diverse applications to be used by future CMPs, as mentioned in Section 2.4. Therefore, to evaluate the validity of our OFDMA interconnect and proposed arbitration algorithms, we have chosen to use synthetic stochastic traffic models. 

To test our algorithms first we start with homogeneous traffic, where we model the number of packets each tileset transmission queue generates every symbol as Poisson process. The most utilized traffic model for synthetic packet generation in NoC literature is the Bernoulli process where at any cycle, a tile or processor generates a packet with probability $p$. As the number of packets that a tileset generates in a symbol is the sum of generated packets by multiple tiles in multiple cycles, it can be modeled as Poisson distribution, because superposition of multiple independent Bernoulli Random Variables yields to a Poisson process.

Hence, firstly, we test our algorithms with \textit{uniformly distributed Poisson Process} case, where total injection rate, \textit{i.e. average number of total packets generated by each tileset on each symbol}, is equally divided among all tilesets (also denoted as ``Uniform Poisson" throughout the thesis). Therefore, the average load of tilesets throughout the simulation is equal, but not the generated packets by them on each symbol as it is a Poisson Process. Of course, this model is far away to provide the necessary benchmark for performance evaluation of our algorithms, due to temporal and spatial heterogeneity of on-chip traffic, which is explained in detail in Section 2.4. 

In order to stress our interconnect and proposed algorithms, as a second stochastic traffic model, we use the \textit{non-uniformly distributed Poisson Process}, where we still use the Poisson process to generate packets by each tileset on each symbol, but total injection rate is distributed non-uniformly. In order to push the limits of performance evaluation, without loss of generality, we set the injection rates of each first 8 tilesets to $1/120$ of total rate; each second 8 tilesets to $2/120$ of total rate; each third 8 tilesets to $4/120$ of total rate and each last 8 tilesets to $8/120$ of total rate. Hence, we make sure certain tilesets generate geometrically larger amount of load on average compared to certain others. One can also loosely relate our chosen ratios to the observations of \cite{soteriou2006statistical}, performed with various applications, that the spatial distribution of total load to processing elements in a CMP follows a Gaussian distribution, with different standard variations depending on the application and architecture. 

In Section 2.4, we have seen that nature of on-chip cache-coherency induces a highly spatial and temporal heterogeneous traffic, that average load generated by certain processing elements shall be significantly different than certain others in CMP. In addition to these, there exists the temporal \textit{self-similar} nature of traffic, which means a tileset generates packets in a \textit{bursty} sense. Of course, the degree of heterogeneity for both temporal and spatial dimensions depends on the type of application, cache sizes, used coherency protocols etc. However, this bursty self-similar temporal nature is a well observed phenomenon for all types of NoCs and CMPs. In Section 2.4, it was mentioned that a \textit{Hurst Parameter} is used to measure the degree of temporal burstiness. Now, to form a third and more realistic traffic model, we seek methods to generate traffic with desired Hurst parameter, $H$. Generation of traffic with accurate $H$ is not trivial and there exists a vast literature for it. \cite{soteriou2006statistical} uses Discrete Fast Fourier Transform (DTFT) to generate packets stochastically for a CMP with NoC with given $H$ as a parameter. Similarly to this work, authors of \cite{bahn2008generic} also propose to generate stochastic traffic for CMPs with given $H$, but by aggregating heavy tailed ON-OFF Pareto Processes. They also propose to employ a recursive mechanism to further tune the traffic's accuracy. This phenomenon, that the aggregation of multiple heavy tailed ON-OFF processes will yield to a approximate self-similar traffic with desired $H$, is well investigated by researchers \cite{pruthi1995heavy}\cite{kramer2000generating}\cite{addie1999modeling}. From a different perspective, in \cite{ledesma2000synthesis}, authors use linear approximation to form fractal Gaussian noise for this purpose. We use as the most feasible one for our case, the method used in \cite{khonsari2008mathematical}, \textit{Discrete Pareto Burst Poisson Process (DPPBP)}, where authors utilized to dimension buffer sizes in a NoC. In this method, each node generates certain number of flows according to a Poisson Process. Then, the length of each generated flow is determined stochastically according to Pareto distribution, which is a discrete number of slots (in our case a symbol). Each flow generates packets at a constant rate (1 packet/symbol in our case) and unlimited number of flows co-exist at any time. Parameters of Poisson and Pareto processes are defined according to desired average load and $H$. In our simulations, in order to stress our algorithms as much as possible, we generally used a $H$ parameter of 0.9 which is much higher than most of applications' empirically derived $H$ parameters in \cite{soteriou2006statistical}. We use this realistic self-similar method with large $H$, to test our algorithms, rather than simulating with existing benchmarks, because current limited number of applications are far away to stress our interconnect, where WiNoCoD aims to serve a vast spectrum of diverse applications in future.  

In addition, we also emulated the bimodal packet sizes of cache coherency. As previously mentioned, authors in \cite{pantuning} found that short control packets constitute between 70\%-80\% of all generated packets, where rest are the cache line carrying long packets. Therefore, we set the ratio of long packets to 25\%. Also, in \cite{kurian2010atac} this ratio for bimodal packets is used. Any generated packet is determined to be either a long or short packet based on this probability, independently. Using information from literature and consulting to memory architectures of WiNoCoD, we set short packet lengths to 64 bits and long packet lengths to 576 bits (where 64 bits for control header, and 512 bits for cache line payload), without loss of generality. These 3 stochastic traffic models are abstracted in Table 4.1.   
 
 
% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[]
\centering
\caption{The parameters of used three stochastic traffic models.}
\label{my-label}
\begin{tabular}{lccccl}
\cline{1-5}
\multicolumn{1}{|l|}{\textit{\textbf{\begin{tabular}[c]{@{}l@{}}Injection \\ Rate\end{tabular}}}}                                                                     & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Tilesets \\ 1-8\end{tabular}}}           & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Tilesets \\ 9-16\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Tilesets \\ 17-24\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Tilesets \\ 25-32\end{tabular}}} &                                               \\ \cline{1-5}
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Uniform \\ Poisson\end{tabular}}                                                                                      & \multicolumn{1}{c|}{1/32}                                                                       & \multicolumn{1}{c|}{1/32}                                                              & \multicolumn{1}{c|}{1/32}                                                               & \multicolumn{1}{c|}{1/32}                                                               &                                               \\ \cline{1-5}
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Non-uniform \\ Poisson\end{tabular}}                                                                                  & \multicolumn{1}{c|}{1/120}                                                                      & \multicolumn{1}{c|}{2/120}                                                             & \multicolumn{1}{c|}{4/120}                                                              & \multicolumn{1}{c|}{8/120}                                                              &                                               \\ \cline{1-5}
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Non-uniform \\ DPBPP\end{tabular}}                                                                                    & \multicolumn{1}{c|}{1/120}                                                                      & \multicolumn{1}{c|}{2/120}                                                             & \multicolumn{1}{c|}{4/120}                                                              & \multicolumn{1}{c|}{8/120}                                                              &                                               \\ \cline{1-5}
\multicolumn{5}{l}{{ }}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          &                                               \\ \cline{1-1}
\multicolumn{1}{|l|}{\textit{\textbf{\begin{tabular}[c]{@{}l@{}}Stochastic \\ number \\ of packets \\ generated on \\ each symbol \\ for each tileset\end{tabular}}}} & \multicolumn{1}{l}{}                                                                            & \multicolumn{1}{r}{\textit{}}                                                          &                                                                                         &                                                                                         &                                               \\ \cline{1-2} \cline{5-6} 
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Uniform \\ Poisson\end{tabular}}                                                                                      & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Poisson \\ Dist.\end{tabular}}                   & \multicolumn{1}{l}{\textbf{}}                                                          & \multicolumn{1}{c|}{\textit{\textbf{}}}                                                 & \multicolumn{1}{c|}{\textit{\textbf{Ratio}}}                                            & \multicolumn{1}{l|}{\textit{\textbf{Length}}} \\ \cline{1-2} \cline{4-6} 
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Non-uniform \\ Poisson\end{tabular}}                                                                                  & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Poisson \\ Dist.\end{tabular}}                   & \multicolumn{1}{l|}{\textbf{}}                                                         & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Short\\ Packets\end{tabular}}}   & \multicolumn{1}{c|}{\%75}                                                               & \multicolumn{1}{l|}{1 flit}                   \\ \cline{1-2} \cline{4-6} 
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Non-uniform \\ DPBPP\end{tabular}}                                                                                    & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}DPBPP\\  method \\ explained \\ in \cite{khonsari2008mathematical} \end{tabular}} & \multicolumn{1}{l|}{\textbf{}}                                                         & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Long\\ Packets\end{tabular}}}    & \multicolumn{1}{c|}{\%25}                                                               & \multicolumn{1}{l|}{9 flits}                  \\ \cline{1-2} \cline{4-6} 
\end{tabular}
\end{table}

 
\section{Using Generic Cores for Bandwidth and Modulation Order Allocation Algorithms}

In this chapter, we have demonstrated a dynamic bandwidth allocation principle, where a dedicated intelligent component composed of one or several processors, is responsible of computing bandwidth allocation algorithms. In case of decentralized allocation, these intelligent units are implanted inside RF front-ends of each tileset, before the IFFT block, executing the same algorithms. In case of centralized allocation, only a single unit is implanted inside one tileset's RF front-end. 

Now, in this section, we introduce the possibility of using few of the generic 2048 cores of the CMP, for the bandwidth allocation and modulation order selection as a concept. As it can be seen from Fig. 4.9, one can utilize, for instance, the core (or several cores) for bandwidth arbitration purposes, inside the tile, which is closest to the RF front-end. This way a low latency communication between this tile and the RF front-end can be attained. The information coming from the local transmission and reception queues, information coming from other tilesets (QSI, utilization of subcarriers etc.) can be transferred to the responsible cores of this tileset, with predefined packet formats. After the necessary computation, the resulting allocation can be re-transferred to the RF front-end, for bandwidth reconfiguration. Modulation order selection algorithms can also be computed via this procedure. 

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.7\textwidth]{./Figures/Chapter8_fig2.pdf}
    \rule{35em}{0.5pt}
  \caption[One or several cores inside the tile, which is closest to the RF Front-end can be utilized to compute banwidth and modulation order allocation algorithms. This implementation can be adopted both for decentralized or centralized approach.]{One or several cores inside the tile, which is closest to the RF Front-end can be utilized to compute banwidth and modulation order allocation algorithms. This implementation can be adopted both for decentralized or centralized approach.} 
  \label{fig:Electron}
\end{figure}

With this approach both decentralized or centralized allocation can be performed. In decentralized case, a tile in each tileset shall be responsible for bandwidth allocation, whereas for centralized case a tile inside just one tileset is necessary. Note that, applying a centralized case for this specific way of bandwidth allocation seems more scalable and efficient, as we only use just one tile. With this approach, we remove the extra area and cost overhead of implanting microprocessors or dedicated circuits inside RF front-ends, for computing allocation algorithms. However, note that, we have not studied the detailed implementation of this option in the scope of this thesis. 

In order to decrease the communication latency further between the RF front-end and the responsible cores, their specific packets containing bandwidth allocation information can be assigned priority inside the mesh router over the other NoC packets coming from other tilesets to RF front-end. 

With this approach, our previously presented pipelined and framed allocation paradigm gains more importance. One can see that, if this approach is adopted for bandwidth allocation, the reconfiguration and computation shall take more time. Therefore, in this case the necessity of a pipelined framework is more underlined. We have examined the proposed allocation algorithms for various frame lengths, hence this may consitute a guideline for this kind of implementation.
\section{Conclusion}

In this chapter, we have defined the goals of this thesis work precisely, by basing on the details of RF interconnect which was given in the previous chapter. It has been emphasized that the main interest of this thesis work for WiNoCoD is to allocate subcarriers to different tilesets to minimize the transmission latency as much as possible. We have justified our methodology for decoupling the effect of lower NoC layers and treat the resource allocation problem solely on the grounds of transmission side inter-tileset communication.

It has been remarked that the very specific constraints and features of the on-chip architecture presented in WiNoCoD, such as very short OFDM symbol duration or strict delay requirements, forces us to treat problem differently. We face an original problem of multiple queues on multiple symbols. We have introduced our frame based pipelined allocation scheme due to unorthodox, short OFDM symbol duration, which mitigates the latency of reconfiguration and other procedures.    

Two different approaches to this defined bandwidth allocation problem have been explained : a centralized allocation, where a single microprocessor(s) or device is responsible for bandwidth arbitration computation or a decentralized allocation, where each tileset has a microprocessor(s) or device to compute the same bandwidth arbitration. Pros and cons of these two complementary approaches have been listed. In addition, all necessary details on signaling and allocation procedures have been defined. 

Finally, the metrics of interest for this thesis work and the stochastic traffic models for performance utilization have been presented. Note that, the information given in this chapter, will constitute the base of the resource allocation algorithms in next chapters. 
