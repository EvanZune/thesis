% Chapter 6

\chapter{Payload Channel Algorithm} % Main chapter title

\label{Chapter6} % For referencing the chapter elsewhere, use \ref{Chapter1} 



\lhead{Chapter 6. \emph{Payload Channel Algorithm}} % This is for the header on each page - perhaps a shortened title

In previous chapter, we have proposed a framework for dynamic bandwidth allocation among tilesets, concerning on-chip time constraints and investigated certain algorithms for arbitrating bandwidth for minimizing transmission latency. However, these algorithms do not take into account the lengths of packets and just consider the instantaneous total number of flits in the queue.  We have noticed that bimodal packet lengths of on-chip cache coherency packets can be exploited in order to further decrease transmission latencies in certain circumstances. Especially, when traffic load is low, so that most of the nodes do not transmit on their allocated bandwidth and cache-lines carrying packets are considerably long, these packets can be transmitted instantly by using under-utilized channels. Based on this principals, we build a new context for exploitation of bimodal packet lengths, taking into account the coordination among tilesets. These new type of algorithms can perform better compared to algorithms proposed in Chapter 5, and also they can be combined together to increase dynamicity further. 


\section{Context}

On-chip cache coherency traffic has an unorthodox nature compared to conventional networks due to its very specific features. As mentioned previously, in Section 2.4, one of these features is \textit{bimodal packet lengths}. We are using a \textit{Distributed Hybrid Directory Based Cache Coherency}, where its detailed description is given in Section 3.1.3. This viable coherence mechanism for 1000-core shared memory architectures imposes high number of broadcast messages, therefore a traffic burden on the wired RF interconnect, which is a ransom to pay for scalability. There are two types of cache coherency packets circulating in a network-on-chip, which are short control packets (i.e. request to read an address line, acknowledgement etc.) and cache-line carrying long packets (i.e. request to write to an address line, response for reading an address line etc.).

In the scope of the project, a short packet length of 64 bits is chosen as feasible. Long packets are composed of a control header, which contain the necessary information such as destination/source ID, packet type etc. and the data in cache line. Cache line size is determined as 64 bytes (512 bits), therefore making long packets 576 bits long. Considering the default QPSK modulation, in case of simple static and uniform allocation of subcarriers (where each tileset acquires 1 RB- i.e. 64 bits), a long packet would take 9 symbols to transmit even under zero queuing delay. Considering 1 OFDM symbol is longer than 50 ns (50 clock cycles if a 1 GHz core frequency is assumed), one can understand the bottleneck long packets create for the on-chip interconnection in terms of transmission latency. Even the basic mathematical and logical operations take several cycles. Considering also into account the OFDM signal reception, decoding, reconfiguration etc., one can see that bandwidth allocation needs multiple symbols to be processed. 

Cache line (block) size has a significant impact on the performance of the shared memory system. Not just large cache size but larger cache lines decrease the cache miss rate generally (the case, a core cannot find referenced copy of required datum in its own L1 cache.) \cite{carter2001schaum}. One of the reasons behind is the spatial locality of the information distribution, so that a fetched larger cache line upon a miss will include more data close to the referenced address, which are likely to be referenced soon. Obviously, there exists a trade-off between the miss rate and the bandwidth overhead, as longer cache line means larger data to traverse through network-on-chip in case of cache miss. Another important point about the performance of chosen cache line size is its mutual dependency on cache size also. For instance, inside a relatively smaller cache, very long cache lines even increase the miss rate \cite{carter2001schaum}. The reason behind this phenomenon is the limited size of data structure of the program. However, this is not the case for every CMPs, where they can exploit spatial locality more effectively. In \cite{bienia2008parsec}, authors had examined the effect of larger cache lines on miss rates for a 8-core CMP and different application from PARSEC benchmark. Certain applications had shown much lower miss rates compared to others with 256 bytes of cache lines, as they are more relying on spatial locality of the data. We can predict that a better performance can be gained via larger cache lines for future shared memory 1000-core architectures and optimized application running on them. Also in case of very large caches are utilized, larger cache lines would be essential. In addition, a reconfigurable, effective low latency interconnect would amortize the penalty of carrying large cache lines, increasing system performance further. In \cite{ono2009adaptive}, it was shown that the benefit of long cache lines as much as 256 bytes is more apparent on shared memory massive multicore architectures, especially for applications with higher locality. 

Taking into account all of these aspects mentioned above, we have developed a novel bandwidth allocation algorithm for WiNoCoD's OFDMA based RF interconnect, which decreases the latency of long packets substantially by transmitting payloads (cache line) in a single symbol. It exploits the intrinsic broadcast capability and elevated reconfigurability of OFDMA. This algorithm is designed especially for large cache lines which may be preferred by the programmer based on the characteristics of the application(s). Different than the used 64 bytes (512 bits) long cache lines throughout the project and the thesis, this algorithm is optimized especially for 256 bytes (2048 bits) long cache lines. In this case, the static algorithm would have to transmit long packets in 33 symbols even under zero queuing (more than 1650 ns) which is highly prohibitive. We also fuse the bandwidth allocation algorithms mentioned in Chapter 5 with this packet size aware policy, in order to increase its performance further under heterogeneous traffic.  

\section{Regular Payload Channel Algorithm}


\subsection{Description of Regular Payload Channel Algorithm}

The proposed algorithm relies on the basic idea of transmitting 2048-bits payloads of long packets in a single OFDM symbol by using all 1024 subcarriers modulated with QPSK. The 64-bit single flit control packets (which corresponds to 70-80\% of all packets) and 64-bit header flit of long packets contain the necessary source/destination ID, corresponding address line etc. information, but also the flag bits marking the type of packet (whether it is a short or long packet). Our novel Payload Channel Algorithm exploits these flag bits to allow transmission of long packet payloads in a single OFDM symbol without using any extra signaling overhead. 

In this proposal, initially, each tileset is allocated 32 subcarriers, where we refer them as \textit{home channels}, with a terminology used in literature as in static case of Section 4.3.1. As QPSK modulation is utilized, these home channels can serve 1 flit per each symbol, either a short control packet or a long packet header. 

Different than the previously presented architecture, for Payload Channel Algorithm, each tileset has two different transmission queues at their RF interfaces. Primary transmission queue buffers short packets or headers of long packets only in a FIFO manner. Whenever a new long packet arrives to RF interface from the interior mesh network, it is segmented into its header (which is buffered in the primary short queue) and its payload (which is buffered in the secondary payload queue in a FIFO manner). This 2-queue architecture is depicted in Fig. 6.1. Motivation behind employing two different queues for headers/short packets and payloads is to avoid the inconsistency between transmitted headers and their payloads and also to allow transmission of a new packet before transmitting a payload of a previous packet. This notion will be explained further in detail within next paragraphs. The idea is to give priority to long packets' payloads, so that they can be transmitted fast, by using other tilesets' allocated channels, especially if they are under-occupied.  


\begin{figure}[htbp]
  \centering
    \includegraphics[width=1.00\textwidth, height= 0.5\textwidth]{./Figures/PayloadChannelAlgorithm01.pdf}
    \rule{35em}{0.5pt}
  \caption[Payload Channel Algorithm Tileset Front-end]{FIFO ``Short Queue" for 1-flit short packets and long packet headers and FIFO ``Long Queue" for payloads at the front-end of each tileset. FIFO short queue is sent on the RF-NoC channel until a long packet header is detected  from any tileset including itself. Then ID of the tileset is inserted to Payload Channel Register. Note that, each tileset keeps the same payload channel register content.}
  \label{fig:Electron}
\end{figure}

When a tileset transmits a long packet header in its home channel, each tileset can acquire and decode this thanks to intrinsic broadcast capability of the OFDMA interconnect. Each tileset can use simple packet processing units to check the flag bits to understand whether this is a long packet or not. Upon tilesets detect a long packet header, they record the corresponding tileset-ID by inserting the ID in the \textit{payload register}. Payload register is a simple FIFO queue at RF interface of each tileset, which buffers IDs of tilesets to transmit payloads. Note that, content of the payload registers is identical in all tilesets, in order to avoid incoherence. 

However, acquiring the header flit, processing it and reconfiguring the bandwidth takes certain amount of time due to propagation, synchronization and computation delays. Therefore, we have determined a 1 symbol latency (50 ns) for all delays and activation of algorithm. Also note that, multiple long packet headers can be detected in a symbol from different tilesets, that their IDs are enregistered to payload registers by their Tileset-No order in the same symbol. 

At the start of each symbol, tilesets control their payload registers. If the payload register is empty, this means there is no tileset currently wanting to transmit a payload. Therefore, home channel configuration is applied and each tileset can use it 64-bit home channel. However, if payload register is not empty, first ID in the register transmits its payload. If it not the tileset's own ID, it does not use its home channel in this symbol to allow the transmission of payload. System returns to home channel configuration only if there is no tileset remains in the payload register. Fig. 6.2 and Fig. 6.3 illustrate the flow-charts of the payload channel algorithm from the view of a transmitter and receiver side of a tileset, respectively. 

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.9\textwidth]{./Figures/Payload_FlowChart_01.pdf}
    \rule{35em}{0.5pt}
  \caption[Flow-chart of regular payload channel algorithm from the view of the transmission side of a tileset.]{Flow-chart of regular payload channel algorithm from the view of the transmission side of a tileset.} 
  \label{fig:Electron}
\end{figure}



\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.9\textwidth]{./Figures/Payload_FlowChart_02.pdf}
    \rule{35em}{0.5pt}
  \caption[Flow-chart of regular payload channel algorithm from the view of the reception side of a tileset.]{Flow-chart of regular payload channel algorithm from the view of the reception side of a tileset.} 
  \label{fig:Electron}
\end{figure}

\subsection{An Illustrative Scenario}


These procedures are shown in Fig. 6.4 with a scenario. In order to explain illustratively, in this example only Tileset-1,2 and 3 are active (rest of the nodes do not transmit any packets). On symbol t=0, Tileset-2 and Tileset-3 transmits a long packet header and Tileset-1 completes the transmission of a short packet. As it takes 1 symbol long latency to receive and process long packet headers, they are registered on symbol t=2 (Note that, this 1 symbol latency includes all delays concerning reception, propagation, synchronization, processing but also enregistering to payload register and reconfiguration of transmission just before the next symbol. In this graphic, for simplicity purposes, register contents are illustrated just at the same time with current symbol). 


\begin{figure}[htbp]
  \centering
    \includegraphics[width=1.00\textwidth, height= 0.8\textwidth]{./Figures/PayloadChannelAlgorithm02_v2.pdf}
    \rule{35em}{0.5pt}
  \caption[Payload Channel Algorithm Illustration]{Illustration of our Payload Channel Algorithm both in frequency and time domain through a simple scenario, where only 3 tilesets are active.}
  \label{fig:Electron}
\end{figure}


As Tileset-2 and Tileset-3 had sent long packet headers, they are enregistered in the payload register with respect to their Tileset-No on the same symbol. Note that, tilesets are still in home channel configuration on symbol t=1, as long packet headers are not processed and Payload Channel Algorithm is not activated, yet. Even though, Tileset-2 has sent a long packet header in previous symbol (and not yet sent the rest of it, i.e. the payload), it can transmit a short packet on symbol t=1. This is possible, thanks to the separate queues for short packets/headers and payloads. On symbol t=2, it can just transmit the payload of the previous packet and at reception, other tilesets can process and perform the necessary desegmentation without inconsistency. 

After Tileset-2's payload is transmitted on symbol t=2, it is deleted from the payload register. On next symbol t=3, Tileset-3 is transmitted. As there is no ID pending left at the payload register, on symbol t=4, system returns to home channel configuration, where tileset can again transmit headers and short control packets. Remark that, they had to stall their transmissions until the transmission of payloads is terminated.

This algorithm is decentralized in its nature and only reconfigurable in terms of payload transmission. One can argue that this algorithm does not provide enough home channels to tilesets with fluctuating bandwidth demands due to heterogeneous on-chip traffic. We will present a ``more dynamic" version of our algorithm in next section, however first, we present experimental results for Regular Payload Channel Algorithm under spatially uniform traffic scenarios (where injection rate of tilesets are equal).

\subsection{Analytic Approximation for Average Latency Calculation}

Before we present the simulation results for average latency induced by our algorithm, we investigate the possibility of deriving a closed form equation for the average latency under specific scenarios. We revisit certain aspects of \textit{Queuing Theory} and develop reasonable approximations. 

First of all, we build a queuing theoretical model of our system as in Fig. 6.5. Assuming a uniform Poisson traffic, any incoming 1-flit packet (long packet header or short packet) to short queue of a tileset obeys a Poisson process. The service time of a short queue is 1 symbol when the payload channel register is empty (home channel configuration). However, for other times service time for this queue is equal to number of tilesets waiting in the payload channel plus 1 symbols, as its transmission is stalled until all current waiting payloads are served. Hence, we can model the short queues at tilesets' front-end as M/G/1 queues based on Kendall's notation (one can refer to \cite{gross1998fundamentals} for the basics of queuing theory and Kendall's notation based queue models). 

\begin{figure}[htbp]
  \centering
    \includegraphics[width=1.00\textwidth, height= 0.8\textwidth]{./Figures/PayloadChannel05.pdf}
    \rule{35em}{0.5pt}
  \caption[Payload Channel Algorithm Analytic Model]{Modeling of Payload Channel Algorithm as a 33 M/G/1 queuing network}
  \label{fig:Electron}
\end{figure}


Next, we try to derive a queuing model for the payload channel register. We can visualize a  Payload Channel Register is serving 1 payload per symbol by using all of the subcarriers, whenever there is a tileset ID in the register. Therefore, service time in payload channel register is ``deterministic" and equal to 1 symbol. Modeling the arrival process to payload register is more complicated. It acquires the long packet headers departing from 32 tileset short queues (M/G/1 queues). Deriving a reasonable departure process model from an M/G/1 queue is not straightforward and requires very complex Markov Process analysis. We do know that departure process from an M/M/1 queue (inter-arrival and service times are exponentially distributed) is a Poisson process \cite{burke1956output}. An interesting article, \cite{mokhtar2000characterization} investigates the characteristics of departure process from an M/G/1 queue with heavy tailed service time distribution and finds that departure process is heavy tailed with the same Hurst parameter. 

As we wish to model the all arrivals to the payload channel register (as an aggregation of 32 tilesets' outputs), rather than individual departure processes, we derive an approximation for it. We know that Poisson distribution is derived as an infinite approximation of binomial for very large number of elements with very small Bernoulli probability \cite{haight1967handbook}. In order to explain in detail, when we have $N$ random values, each independently and identically distributed (i.i.d.) and can be 1 with probability $p$ or 0 otherwise. When $N$ is very large (going to infinite) and $p$ is very small (going to zero), the probability distribution of the number of 1's at any time is a Poisson distribution. Our case is similar to this scenario for long packet header arrivals to payload channel, as we have 32 random variables, each can be '1' (i.e. long packet header is transmitted) with a certain probability. Although either $N$ is not very large (32) or $p$ is very small (probability of having a long packet header), we think modeling the arrivals to the payload channel register as a Poisson Process seems highly reasonable. Therefore, at the end, we have modeled the Payload Channel Register as an M/D/1 queue. We may guess that this approximation would be much valid when injection rate is low, lowering $p$.

Firstly, we derive the service time probability distribution for the 1-flit packets (either long packet headers or short control packets) in the short queues at tilesets' front-ends, $S_{short}$. As noted previously, service time for short packets is equal to 1+$L_{payload}$ (current number of elements waiting in payload register). We have mentioned that payload register is modeled as an M/D/1 queue and we have to have its Queue Length distribution in order to derive service time distribution of short packets. This distribution was derived for the special case where service time is equal to 1 unit, which is exactly the same case for the payload register \cite{nakagawa2005series}. By adding 1 symbol to these equations in \cite{nakagawa2005series}, as service time in a short queue is 1 plus the number of elements waiting in the register, we can rewrite the distribution for the service time of short packets as  : 



\[ \textrm{$p(S_{short}) = $}
   \begin{cases}  
   
     \textrm{$1-\lambda_{payload}$}  & \textrm{\, \, if $S_{short} = 1$} \\
      \textrm{$(1-\lambda_{payload})(e^{\lambda_{payload}}-1)$}  & \textrm{\, \, if $S_{short} = 2$}\\
      \textrm{...}  & \, \, ...\\
     \textrm{$(1-\lambda_{payload})\bigg(e^{(n-1)\lambda_{payload}}+ \sum\limits_{k=1}^{n-2}e^{k\lambda_{payload}}(-1)^{(n-k-1)}$}  & \\
     
    \textrm{$\bigg[\frac{(k\lambda)^{(n-k-1)}}{(n-k-1)!} + \frac{(k\lambda)^{(n-k-2)}}{(n-k-2)!}  \bigg] \bigg)$}  & \textrm{\, \, if $S_{short} = n$}
   
   \end{cases} 
\]

where $\lambda_{payload}$ is the mean arrival rate (i.e. injection rate) to the payload register. Note that, $\lambda_{payload} = \lambda p_{long}$, i.e. total injection rate multiplied by the proportion of long packets in the system. Here, for instance, for a 1-flit packet to be served in the short queue in 1 symbol (either a header of a long packet or a 1-flit short packet), the payload queue has to be empty when the packet has arrived. In other words there should not be any long packet transmission, so that the home channels can be used and a short packet can be served in 1 symbol. This probability equals to $1-\lambda_{payload}$, as this is also equal to the probability of payload queue to be empty. As service time for a short packet, $S_{short}$ increases, the associated probability, $p(S_{short}) = $, increases with exponentially (and oscillatory due to Taylor's series expansion) with service time and average input to the payload queue. 

We can derive the mean of service time of short packets from this analytical distribution as : 

\begin{align}
\mu_{short} = \sum\limits_{i=1}^{\infty} ip(S_{short} = i)  
\end{align}

and using (6.1) and taken M/D/1 distribution, we can also derive the variance as :

\begin{align}
Var(S_{short}) = \sum\limits_{i=1}^{\infty} (i - \mu_{short})^{2}p(S_{short} = i)  
\end{align}

Having the probability of service time distribution of short packets (either long packet headers or 1-flit control packets), we can continue to derive the average queuing plus service time in M/G/1 modeled short queues in tilesets' interface using Pollaczek-Khinchine Formula \cite{khinchin1967mathematical} :

\begin{align}
W_{short} = \frac{\rho_{short}+\lambda_{short}\mu_{short}Var(S_{short})}{2(\mu_{short}-\lambda_{short})}+\frac{1}{\mu_{short}}
\end{align}

where $\mu_{short}$ (mean service rate) is the reciprocal of mean service time of short packets which can be derived by taking the expectation over the service time distribution given previously. Similarly variance of the service time can also be calculated from this distribution. $\rho_{short}$ signifies the utilization in these short queues which is equal to $\lambda_{short}/\mu_{short}$. $\lambda_{short}$ is simply the average injection rate to each of the short queues which is total injection rate divided by the number of tilesets, as both long packet headers and short control packets arrive to these queues.


In parallel, as long packets are composed of a header firstly served at short queue and a payload served by the payload register queue model; average queuing plus service time of a long packet can be calculated as the sum of the average queuing plus service time of a short packet and payload : $W_{long} = W_{short} + W_{payload}$. 

We have stated previously that payload register queue is simply an M/D/1 model where D=1. Thus, its average queuing plus service time (average delay) can easily be calculated from the  Pollaczek-Khinchine Formula : 

\begin{align}
W_{payload} = \frac{2-\lambda_{payload}}{2(1-\lambda_{payload})}
\end{align}


At the end, we can write the approximated average delay in the system as the proportional sum of long and short packets as a function of total injection :

\begin{align}
W = (1- p_{long})W_{short} + p_{long}W_{long}
\end{align}

Due to the Taylor series expansion based derivation of the stationary distribution of M/D/1 queue, we cannot write down the resulting average latency formula in closed from here, as it requires too much space. The analytic approximation is calculated by Matlab using loops for series. Note that, in definition these series expansions go to infinity in alternating directions. Using too large expansions may cause instabilities, therefore we have calculated these values up to several hundreds. In addition, probability of having too large values for the service of short packets, such as larger than 100 symbols, is too small. 
 
\subsection{Experimental Results for Payload Channel Algorithm}

We test the performance of our payload channel algorithm by comparing it to the scenario where bandwidth is allocated statically and equal among tilesets. Understandably, this version of the payload channel algorithm is only tested for the uniform Poisson and uniform burst traffic models.

\textit{Average Latency}


Fig. 6.6 shows the average latency with increasing total injection rate under uniform Poisson traffic, along with the our theoretically derived approximation. Note that, thanks to its effective reconfiguration to transmit payloads in single symbols it can provide up to 10 times lesser average latency, compared to static case. Also note that, our queuing theory approximation well fits to resulting average delay, except for large injection rates close to system limit. This shall root from the success of Poisson distribution with low injection rate, as previously explained. Results provide a slightly lower injection rate limit compared to theoretical one, which we can speculate that it is due to higher variance of service and arrival rates compared to Poisson distribution (higher variance increases average latency \cite{khinchin1967mathematical}). Also, as we are considering 256 bytes long payloads for long packets now, the average length of a packet is 8.75 flits, which explains the new system limit for static allocation Fig. 6.6, injection rate approximately equal to 32/8.75 = 3.6 packets/symbol.  

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.7\textwidth]{./Chapter6_Figures/PayloadChannel_AvgLatency_Uniform_v2.eps}
    \rule{35em}{0.5pt}
  \caption[Payload Channel Average latency under uniform Poisson]{Average latency under uniform Poisson traffic with increasing injection rate for Payload Channel Algorithm, analytic approximation and the reference static case.}
  \label{fig:Electron}
\end{figure}

Fig. 6.7 shows the same performance measure for our algorithm and reference static and dynamic QPS allocation under uniform DPBPP traffic with H=0.9 presenting a much higher temporal heterogeneity. Note that, even under this much of temporal burstiness, our algorithm is still able to induce up to x10 less average latency, thanks to its separate payload queues and novel subcarrier reconfiguration without requiring any signaling overhead. 

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.7\textwidth]{./Chapter6_Figures/PayloadChannel_AvgLatency_DPPBP_v2.eps}
    \rule{35em}{0.5pt}
  \caption[Payload Channel Average latency under uniform DPBPP traffic (H=0.9) ]{Average latency under uniform DPBPP traffic (H=0.9) with increasing injection rate for Payload Channel Algorithm and the reference static case.}
  \label{fig:Electron}
\end{figure}

\textit{Packet Delay and Queue Length Bound Exceeding Probabilities}


Next, we investigate the probability of exceeding given delays for packets for our payload channel algorithm and the reference static algorithm. 

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.7\textwidth]{./Chapter6_Figures/PayloadChannel_DelayExceedProb_Uniform_v2.eps}
    \rule{35em}{0.5pt}
  \caption[Delay exceeding probability graphs for our payload channel algorithm under uniform traffic]{Delay exceeding probability graphs for our payload channel algorithm under uniform Poisson and DPPBP (H=0.9) traffic}
  \label{fig:Electron}
\end{figure}

Fig. 6.8 shows the delay exceeding probabilities under 2 uniform policies, highlighting the absolute performance gain of our payload channel algorithm. For instance, under uniform Poisson traffic, the probability of a packet to have a delay larger than 30 symbols is x100 lesser compared to static allocation. Under uniform DPBPP traffic, with introduced burstiness our algorithm's performance degrades significantly, but it still provides approximately x5 times lesser probability to exceed a delay of 30 symbols. 


\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.7\textwidth]{./Chapter6_Figures/PayloadChannel_QueueExceedProb_UniformPoisson_v2.eps}
    \rule{35em}{0.5pt}
  \caption[Queue Length exceeding probability graphs for our payload channel algorithm under uniform Poisson traffic]{Queue Length exceeding probability graphs for our payload channel algorithm under uniform Poisson traffic}
  \label{fig:Electron}
\end{figure}

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.7\textwidth]{./Chapter6_Figures/PayloadChannel_QueueExceedProb_UniformDPPBP_v2.eps}
    \rule{35em}{0.5pt}
  \caption[Queue Length exceeding probability graphs for our payload channel algorithm under uniform DPBPP traffic]{Queue Length exceeding probability graphs for our payload channel algorithm under uniform DPBPP traffic (H=0.9)}
  \label{fig:Electron}
\end{figure}

\begin{figure}[htbp]
  \centering
    \includegraphics[width=1.00\textwidth, height= 0.8\textwidth]{./Figures/PayloadChannelAlgorithm03.pdf}
    \rule{35em}{0.5pt}
  \caption[Frame structure and centralized bandwidth allocation mechanism of the proposed dynamic payload channel algorithm]{Frame structure and centralized bandwidth allocation mechanism of the proposed dynamic payload channel algorithm}
  \label{fig:Electron}
\end{figure}

And finally the queue exceeding probabilities are compared. Payload channel algorithm requires 3 separate queues to be install at each tileset's front-end : a short queue for control packets and long packet headers, a payload queue for 32-flit long payloads and a payload register to keep the ID of the tilesets dynamically, demanding to use payload channel  (which stores the copy of the same information all time at each tileset). And conventional static allocation requires only a single queue. Note that, given queue lengths are in terms of units, that for payload queue it is the number of payloads (means one has to multiply it by 32 in order to calculate the capacity in terms of flits), for payload register it is the current number of tileset IDs (as there are 32 tilesets we need 5 bits storage to represent each). 

Fig. 6.9 and Fig. 6.10 shows the queue length exceeding probabilities under uniform Poisson and DPBPP traffic, respectively. Let us remark that, in Fig. 6.9 and Fig. 6.10, the x-axis does not represent the number of flits, particularly. For payload queue, x-axis represents the number of payloads of 32 flits long (cache line); for short packet queue, it represents the number of 1-flit long short packets; for payload register, it represents the number of IDs in a payload register at a time (for 32 tilesets, it is 5-bit long number) and for reference algorithm, it represents the number of flits in a transmission queue as in Chapter 5. We have combined these different indicators in a single graph in order to provide a conceptual coherence. One can dimension and compare the required memory for each of these 3 different queues for payload channel algorithm by looking at these probabilities of exceeding. For instance, in Fig. 6.10, with a probability of $10^{-2}$, for the payload channel algorithm, the number of payloads (where each is 32 flits long) in the payload queue exceeds 8 (256 flits), for the short packet queue (where each is 1 flit), the number of packets exceeds 30 (30 flits) and finally the instantaneous number of tileset-IDs waiting (where each is 5 bits for 32 tilesets) exceeds 10 with this probability. For the same probability of $10^{-2}$, for instance reference dynamic allocation algorithm with a single queue, the instantaneous number of flits can only exceed 90, where for payload channel algorithm total required capacity is 286 flits. As you can see even though, the proposed payload channel algorithm can provide less average latency compared to reference, it may require higher buffer capacity, as we need to store 32 flits long payload for relatively longer durations.   
     
One can see the reasonably high performance of our algorithm in terms of requiring small buffer lengths, even under the spikes of bursty traffic. Most importantly, payload queue, where each element is composed of 2048 bits, does not require prohibitively large capacity, thanks to effectiveness of our algorithm. 


\section{Dynamic Payload Channel Algorithm}

\subsection{Description of Dynamic Payload Channel Algorithm}

Spatial imbalance of cache coherence traffic was discussed intensively through the thesis manuscript. With additional temporal burstiness, our payload channel algorithm needs a modification in order to cope with this situation. For this purpose, we develop Dynamic Payload Channel Algorithm by merging previously mentioned payload channel algorithm with centralized queue length proportional (QPS) algorithm. 

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.9\textwidth]{./Figures/Payload_FlowChart_03.pdf}
    \rule{35em}{0.5pt}
  \caption[Flow-chart of dynamic payload channel algorithm from the view of the transmission side of a tileset.]{Flow-chart of dynamic payload channel algorithm from the view of the transmission side of a tileset.} 
  \label{fig:Electron}
\end{figure}

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.9\textwidth]{./Figures/Payload_FlowChart_07.pdf}
    \rule{35em}{0.5pt}
  \caption[Flow-chart of dynamic payload channel algorithm from the view of the receiver side of a tileset.]{Flow-chart of dynamic payload channel algorithm from the view of the receiver side of a tileset.} 
  \label{fig:Electron}
\end{figure}

The idea is to change the number of home channels of each tileset has within a frame, by the help of a central intelligent unit (CIU) as in previous dynamic algorithm mechanisms. Similarly, on the first symbol of the each frame, each tileset encodes its QSI. CIU, calculating expected QSIs (EQSI) of tilesets, it allocates the RBs (i.e. home channels in this case) to tilesets according to QPS algorithm. Different than the conventional QPS algorithm, the symbols where payloads are being transmitted is not counted, while determining the length of a frame. Therefore, length of a frame is indefinitive. However, its minimum value, where payload channel is never used, is defined by the required computation and reconfiguration time as in QPS algorithm. Based on the response messages sent by CIU on pre-determined symbol and subcarriers, tilesets reconfigure their arbitration for home channels in next frame. This procedure is illustrated in detail in Fig. 6.11. 


Fig. 6.12 and Fig. 6.13 illustrate the flow-charts of the dynamic payload channel algorithm from the view of a transmitter and receiver side of a tileset, respectively. 


\begin{figure}[htbp]
  \centering
    \includegraphics[width=1.00\textwidth, height= 0.8\textwidth]{./Figures/PayloadChannelAlgorithm04_v2.pdf}
    \rule{35em}{0.5pt}
  \caption[Illustration of our Dynamic Payload Channel Algorithm both in frequency and time domain through a simple scenario, where only 3 tilesets are active.]{Illustration of our Dynamic Payload Channel Algorithm both in frequency and time domain through a simple scenario, where only 3 tilesets are active.}
  \label{fig:Electron}
\end{figure}

Note that, encoded QSIs in this case do not represent the real number of flits waiting in the front-end, but the number of packets (short and long), in other words the elements in the short queue. This is due to fact that, each time payload channel is being used it serves one payload and by allocating home channels proportionally to the number of all packets. We make sure that bandwidth demand is shared proportionally, as each home channel serves a short packet or give access to the payload channel for a long packet.  



\subsubsection{Illustrative Scenario}


Fig. 6.14 shows a simple illustrative scenario for dynamic payload channel algorithm where only 3 tilesets are active. For instance, on symbol t=0, Tileset-1 has 3 home channels, where it has transmitted 2 long packet headers and a short packet. As mentioned previously, thanks to separate payload channel queues, inconsistency between long packet headers and payloads is resolved. Therefore, a tileset may also send multiple long packet header on the same symbol and stall the transmission of payloads in payload channel queue, while continuing to use next home channels it acquired. Due to the system latency, both of these payload channel requests are activated on symbol t=2. As Tileset-1 has sent 2 payload channel requests, in the payload channel register its ID is inserted twice. Then, on symbol t=2 and t=3 it transmits the payloads of these 2 packets. And on symbol t=4, the configuration of tilesets' home channels are changed due to distribution in the frame. 

\subsection{Experimental Results for Dynamic Payload Channel Algorithm}

We test the performance of our proposed dynamic payload channel algorithm for non-uniform Poisson and DPBPP traffic. In order to ensure a reliable competence, we compare the results to the basic centralized QPS algorithm mention in 5.2, where we also base our algorithm on. 

\textit{Average Latency}


\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.7\textwidth]{./Chapter5_Figures/PayloadChannel_AvgLatency_NonUniformPoiss_BandwidthAllocation_copy.eps}
    \rule{35em}{0.5pt}
  \caption[Dynamic Payload Channel Average latency under uniform Poisson Traffic ]{Average latency under non-uniform Poisson traffic with increasing injection rate for Payload Channel Algorithm and the reference basic QPS allocation.}
  \label{fig:Electron}
\end{figure}


\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.7\textwidth]{./Chapter5_Figures/PayloadChannel_AvgLatency_NonUniformDPPBP_BandwidthAllocation_copy.eps}
    \rule{35em}{0.5pt}
  \caption[Dynamic Payload Channel Average latency under non-uniform DPBPP traffic (H=0.9) ]{Average latency under non-uniform DPBPP traffic (H=0.9) with increasing injection rate for Payload Channel Algorithm and the reference basic QPS allocation.}
  \label{fig:Electron}
\end{figure}



Fig. 6.15 and Fig. 6.16 shows the average latency with increasing total injection rate under non-uniform Poisson and DPBPP (H=0.9) traffic, respectively. The most interesting observation from these points is the better performance of conventional QPS algorithm compared to Dynamic Payload Channel Algorithm after a certain injection rate (For non-uniform Poisson around 2 packets/symbol and for non-uniform DPBPP around 2.5 packets/symbol). This shall root from the requirement of more rapid response to the fluctuations of QSIs under higher injection rates. In higher traffic load, dynamic payload channel algorithm induces more and more payload channel utilization, which disrupts the default home channel configuration and also makes frame lengths much longer (as payload channel symbols are not counted in frame length). This causes to reconfiguration of subcarriers much less frequently and with outdated QSI. However, for low injection rates, the performance of the payload channel algorithm is evident, especially for Poisson traffic. 


\textit{Packet Delay and Queue Length Bound Exceeding Probabilities}


Next, we evaluate the delay and queue length exceeding probability graphs, as usual. Fig. 6.17 shows the delay exceeding probability for proposed dynamic payload channel algorithm and the normal QPS algorithm, under both non-uniform Poisson and DPBPP (H=0.9) traffic with a total injection rate of 3 packets/symbol. Interestingly, we see some better performance (lower probability of exceeding) of conventional QPS compared to dynamic payload channel algorithm, especially for packet delays larger than 30 symbols. Thus, we can deduce dynamic payload channel algorithm may induce larger delays under relatively high load of traffic. However, we do have shown the better performance of the proposed payload channel algorithm for lower traffic rates.   

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.7\textwidth]{./Chapter5_Figures/PayloadChannel_DelayExceedProb_NonUniform_copy.eps}
    \rule{35em}{0.5pt}
  \caption[Delay exceeding probability graphs for our dynamic payload channel algorithm under uniform DPBPP traffic]{Delay exceeding probability graphs for our payload channel algorithm under non-uniform Poisson and DPBPP traffic (H=0.9) compared to reference QPS algorithm}
  \label{fig:Electron}
\end{figure}

Fig. 6.18 and 6.19 shows the queue length exceeding probability graphs for dynamic payload channel algorithm and conventional QPS under non-uniform Poisson and DPBPP (H=0.9) traffic, respectively. Let us remark that, in Fig. 6.18 and Fig. 6.19, the x-axis does not represent the number of flits, particularly. For payload queue, x-axis represents the number of payloads of 32 flits long (cache line); for short packet queue, it represents the number of 1-flit long short packets; for payload register, it represents the number of IDs in a payload register at a time (for 32 tilesets, it is 5-bit long number) and for reference algorithm, it represents the number of flits in a transmission queue as in Chapter 5. We have combined these different indicators in a single graph in order to provide a conceptual coherence. One can dimension and compare the required memory for each of these 3 different queues for payload channel algorithm by looking at these probabilities of exceeding.        

We observe that, even for the relatively high bursty input traffic of 3 packets/symbol rate, 1-flit short queue exceeds the 80 flits capacity with a probability around $10^{-4}$. Compared to reference conventional QPS, the real bottleneck is the payload queues, as each element is 32 flits long. However, even for a 3 packets/symbol DPBPP traffic, number of payloads being buffered in a tilesets payload queue exceeds 10 with only a probability of $10^{-4}$, which shows the feasibility and robustness of the proposed algorithm.   

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.7\textwidth]{./Chapter5_Figures/PayloadChannel_QueueExceedProb_NonUniformPoisson_copy.eps}
    \rule{35em}{0.5pt}
  \caption[Queue length exceeding probability graphs for our payload channel algorithm under non-uniform Poisson traffic compared to reference QPS algorithm]{Queue length exceeding probability graphs for our payload channel algorithm under non-uniform Poisson traffic compared to reference QPS algorithm}
  \label{fig:Electron}
\end{figure}

\begin{figure}[htbp]
  \centering
    \includegraphics[width=0.7\textwidth]{./Chapter5_Figures/PayloadChannel_QueueExceedProb_NonUniformDPBPP_copy.eps}
    \rule{35em}{0.5pt}
  \caption[Queue length exceeding probability graphs for our payload channel algorithm under non-uniform DPBPP traffic (H=0.9) compared to reference QPS algorithm]{Queue length exceeding probability graphs for our payload channel algorithm under non-uniform DPBPP traffic (H=0.9) compared to reference QPS algorithm}
  \label{fig:Electron}
\end{figure}


\section{Conclusion}


We have presented an innovative bandwidth allocation mechanism called ``Payload Channel Algorithm", which exploits the bimodal nature of on-chip coherence packets. To the best of our knowledge, this is the first approach of this kind. Using strong reconfigurability and broadcast support of OFDMA, proposed algorithm can decrease average latency up to 20 times compared to a static one, under certain specific scenarios. This novel algorithm requires no additional signaling overhead, and is implemented with significantly low complexity. The algorithm is specially intended for the on-chip architectures with longer cache lines. We have also presented a dynamic version of this algorithm, by merging a centralized bandwidth allocation proposed in Chapter 5; which adapts bandwidth allocation according to spatial traffic fluctuations. We believe proposed Payload Channel Algorithm would constitute an important pioneer work for the management of bimodal on-chip packets. 